#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
run_eval_dialogue_cycle.py
=========================
å®ç°å¦‚ä¸‹é—­ç¯ï¼š
  0) åˆå§‹ï¼šrun_full_cycle.py ç”Ÿæˆæ–°ç†è®ºå¹¶è¯„ä¼°
     â””â–º å¾—åˆ° eval_ready_theories + final_evaluation_summary.json
     â””â–º ç«‹å³æ‰§è¡Œ run_m3_auto_refinement.py åšé¦–è½®å¯¹è¯æ”¹å†™

  1..N) è¿­ä»£ï¼š
     a) demo/demo_1.py  å¯¹å½“å‰ç†è®ºæ± é‡æ–°è¯„ä¼°ï¼Œç”Ÿæˆæ–°çš„ summary
     b) run_m3_auto_refinement.py  å¤šè½®å¯¹è¯æ”¹å†™
     c) ç”¨æ”¹å†™ç‰ˆæœ¬æ›´æ–°ç†è®ºæ± ï¼Œè®¡ç®—å¹³å‡æå‡
     d) è‹¥ avg_delta < stop_delta (é»˜è®¤ 0.03) æˆ–è¾¾åˆ°æœ€å¤§ä»£æ•°åˆ™åœæ­¢

ç”¨æ³•ç¤ºä¾‹ï¼š
python run_eval_dialogue_cycle.py \
  --generations 1 \
  --initial_theories_dir data/theories_v2.1 \
  --output_root data/dialog_cycle_runs_test_gemini \
  --max_pairs_to_analyze 1 \
  --variants_per_contradiction 1 \
  --top_n 1 --max_iters 1 --stop_delta 0.03 \
  --role_eval_threshold 0.5 \
  --synthesis_model_source google  --synthesis_model_name gemini-2.5-pro-preview-06-05 \
  --evaluation_model_source google --evaluation_model_name gemini-2.5-pro-preview-06-05 \
  --dialog_model_source google     --dialog_model_name gemini-2.5-pro-preview-06-05
"""

from __future__ import annotations
import subprocess
import sys
import argparse
from pathlib import Path
import glob
import json
import shutil
import time


# -----------------------------------------------------------------------------
# å·¥å…·å‡½æ•°
# -----------------------------------------------------------------------------

def run_cmd(cmd: list[str]):
    print("\n$ " + " ".join(cmd))
    ret = subprocess.call(cmd)
    if ret != 0:
        print(f"[FATAL] å‘½ä»¤å¤±è´¥: {' '.join(cmd)}")
        sys.exit(ret)


def find_unique(pattern: str) -> Path:
    matches = glob.glob(pattern)
    if not matches:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°åŒ¹é…: {pattern}")
    if len(matches) > 1:
        raise RuntimeError(f"åŒ¹é…åˆ°å¤šä¸ªè·¯å¾„: {matches}")
    return Path(matches[0])


def collect_improved(depth_runs_dir: Path, dest_dir: Path):
    """å¤åˆ¶ depth_runs ä¸­æœ€å iter çš„ improved_*.json åˆ° dest_dirï¼Œè¿”å›ç†è®ºåŠå…¶ delta åˆ—è¡¨"""
    from tools.refinement.schema_validator import is_valid_theory_json

    dest_dir.mkdir(parents=True, exist_ok=True)
    deltas = []

    for theory_dir in depth_runs_dir.iterdir():
        if not theory_dir.is_dir():
            continue
        # è¯»å– summary
        summary_file = theory_dir / "summary.json"
        if not summary_file.exists():
            continue
        with open(summary_file, "r", encoding="utf-8") as f:
            sdata = json.load(f)
        baseline = sdata["scores"][0]
        final_score = sdata["final_score"]
        deltas.append(final_score - baseline)

        # æ‰¾ improved æ–‡ä»¶
        iter_dirs = sorted([p for p in theory_dir.iterdir() if p.is_dir() and p.name.startswith("iter_")],
                           key=lambda p: int(p.name.split("_")[1]))
        last_iter = iter_dirs[-1] if iter_dirs else None
        src_json = None
        if last_iter:
            improved = list(last_iter.glob("improved_*.json"))
            if improved:
                src_json = improved[0]
        if src_json is None:
            continue
        # æ ¡éªŒ
        with open(src_json, "r", encoding="utf-8") as f:
            tdata = json.load(f)
        if not is_valid_theory_json(tdata):
            continue
        # ç»Ÿä¸€æ–‡ä»¶å‘½åï¼Œé¿å…ä¸åŒé˜¶æ®µå­—ç¬¦ä¸²æ›¿æ¢è§„åˆ™ä¸ä¸€è‡´
        try:
            # ä¸ candidate_selector.slugify ä¿æŒä¸€è‡´
            from tools.refinement.candidate_selector import slugify  # type: ignore
        except ImportError:
            # å›é€€ï¼šç®€å•æ›¿æ¢éæ³•å­—ç¬¦
            import re

            def slugify(text: str) -> str:
                return re.sub(r"[^0-9a-zA-Z]+", "_", text).strip("_")[:120]

        slug_name = slugify(tdata.get("name", theory_dir.name)) + ".json"
        shutil.copy(src_json, dest_dir / slug_name)
    # è®¡ç®—å¹³å‡æå‡
    avg_delta = sum(deltas)/len(deltas) if deltas else 0
    return avg_delta, len(deltas)

# -----------------------------------------------------------------------------
# ä¸»é€»è¾‘
# -----------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser("è¯„ä¼°â†”å¯¹è¯å¤šè½®å¾ªç¯æ§åˆ¶è„šæœ¬")
    parser.add_argument("--generations", type=int, default=5, help="æœ€å¤§å¾ªç¯ä»£æ•°ï¼ˆå«é¦–ä»£ï¼‰")
    parser.add_argument("--initial_theories_dir", required=True, help="åˆå§‹å·²æœ‰ç†è®ºæ± ç›®å½•")
    parser.add_argument("--output_root", default="data/dialog_cycle_runs", help="æ€»è¾“å‡ºæ ¹ç›®å½•")

    parser.add_argument("--stop_delta", type=float, default=0.03, help="å¹³å‡æå‡ä½äºè¯¥é˜ˆå€¼åˆ™æå‰åœæ­¢")

    # --- ç”Ÿæˆé˜¶æ®µå‚æ•° ---
    parser.add_argument("--max_pairs_to_analyze", type=int, default=5, help="run_direct_synthesis.py: åˆ†æçš„æœ€å¤§çŸ›ç›¾å¯¹æ•°")
    parser.add_argument("--variants_per_contradiction", type=int, default=1, help="run_direct_synthesis.py: æ¯ä¸ªçŸ›ç›¾ç”Ÿæˆçš„å˜ä½“æ•°")
    parser.add_argument("--synthesis_model_source", default="google")
    parser.add_argument("--synthesis_model_name", default="gemini-1.5-pro-latest")

    # --- è¯„ä¼°é˜¶æ®µ LLM å‚æ•°ï¼ˆJudgeï¼‰---
    parser.add_argument("--evaluation_model_source", default="deepseek")
    parser.add_argument("--evaluation_model_name", default="deepseek-reasoner")

    # --- å¯¹è¯ä¼˜åŒ–é˜¶æ®µ LLM å‚æ•° ---
    parser.add_argument("--dialog_model_source", default="deepseek")
    parser.add_argument("--dialog_model_name", default="deepseek-reasoner")

    parser.add_argument("--top_n", type=int, default=3)
    parser.add_argument("--max_iters", type=int, default=2)
    parser.add_argument("--min_improve", type=float, default=0.03)

    # --- è§’è‰²è¯„ä¼°é˜ˆå€¼ ---
    parser.add_argument("--role_eval_threshold", type=float, default=0.6, help="å®éªŒæˆåŠŸç‡è¾¾åˆ°è¯¥å€¼æ‰è¿›è¡Œè§’è‰²è¯„ä¼° (é€ä¼ ç»™ run_full_cycle.py)")

    args = parser.parse_args()

    root = Path(args.output_root).resolve()
    root.mkdir(parents=True, exist_ok=True)

    # ---------------- Generation 0 -----------------
    gen0_dir = root / "generation_0"
    gen0_dir.mkdir(exist_ok=True)

    # Phase 0A: run_full_cycle
    full_cycle_dir = gen0_dir / "full_cycle"
    cmd_full = [
        "python", "run_full_cycle.py",
        "--existing_theories_dir", args.initial_theories_dir,
        "--base_output_dir", str(full_cycle_dir),
        "--max_pairs_to_analyze", str(args.max_pairs_to_analyze),
        "--variants_per_contradiction", str(args.variants_per_contradiction),
        "--synthesis_model_source", args.synthesis_model_source,
        "--synthesis_model_name", args.synthesis_model_name,
        "--evaluation_model_source", args.evaluation_model_source,
        "--evaluation_model_name", args.evaluation_model_name,
        "--role_eval_threshold", str(args.role_eval_threshold),
        "--synthesis_model_source", args.synthesis_model_source,
        "--synthesis_model_name", args.synthesis_model_name
    ]
    run_cmd(cmd_full)

    # å®šä½è·¯å¾„
    run_subdir = find_unique(str(full_cycle_dir / "run_*/"))
    summary_file = find_unique(str(run_subdir / "2_evaluation_output" / "*/final_evaluation_summary.json"))
    theories_root = find_unique(str(run_subdir / "1_synthesis_output" / "*/eval_ready_theories"))

    # Phase 0B: é¦–è½®å¯¹è¯æ”¹å†™
    refinement_dir = gen0_dir / "refinement"
    cmd_refine0 = [
        "python", "run_m3_auto_refinement.py",
        "--summary_file", str(summary_file),
        "--theories_root", str(theories_root),
        "--output_dir", str(refinement_dir),
        "--top_n", str(args.top_n),
        "--eval_mode", "real",
        "--max_iters", str(args.max_iters),
        "--min_improve", str(args.min_improve),
        "--judge_model_source", args.evaluation_model_source,
        "--judge_model_name", args.evaluation_model_name,
        "--dialog_model_source", args.dialog_model_source,
        "--dialog_model_name", args.dialog_model_name
    ]
    run_cmd(cmd_refine0)

    # å…¼å®¹ run_m3_auto_refinement çš„è¾“å‡ºç»“æ„ï¼šå¯èƒ½å¸¦ run_*/
    candidates = list(refinement_dir.glob("run_*/depth_output/depth_runs"))
    depth_runs_dir = candidates[0] if candidates else (refinement_dir / "depth_output/depth_runs")

    pool_dir = gen0_dir / "theory_pool"
    avg_delta, count = collect_improved(depth_runs_dir, pool_dir)
    print(f"[GEN0] æ”¶é›† {count} ä¸ªæ”¹å†™ç†è®ºï¼Œå¹³å‡æå‡ {avg_delta:.3f}")

    # ---------------- Subsequent Generations -----------------
    theories_dir_for_next = pool_dir

    for gen in range(1, args.generations):
        print("\n" + "="*80)
        print(f"Generation {gen}")
        print("="*80)
        gen_dir = root / f"generation_{gen}"
        gen_dir.mkdir(exist_ok=True)

        # Phase A: é‡æ–°è¯„ä¼°
        eval_dir = gen_dir / "evaluation"
        eval_dir.mkdir(exist_ok=True)
        cmd_eval = [
            "python", "demo/demo_1.py",
            "--theory_path", str(theories_dir_for_next),
            "--experiment_dir", "demo/experiments/",
            "--output_dir", str(eval_dir),
            "--model_source", args.evaluation_model_source,
            "--model_name", args.evaluation_model_name,
            "--run_role_evaluation",
            "--role_model_source", args.evaluation_model_source,
            "--role_model_name", args.evaluation_model_name
        ]
        run_cmd(cmd_eval)
        summary_file = find_unique(str(eval_dir / "run_*/final_evaluation_summary.json"))

        # Phase B: å¯¹è¯æ”¹å†™
        refinement_dir = gen_dir / "refinement"
        cmd_refine = [
            "python", "run_m3_auto_refinement.py",
            "--summary_file", str(summary_file),
            "--theories_root", str(theories_dir_for_next),
            "--output_dir", str(refinement_dir),
            "--top_n", str(args.top_n),
            "--eval_mode", "real",
            "--max_iters", str(args.max_iters),
            "--min_improve", str(args.min_improve),
            "--judge_model_source", args.evaluation_model_source,
            "--judge_model_name", args.evaluation_model_name,
            "--dialog_model_source", args.dialog_model_source,
            "--dialog_model_name", args.dialog_model_name
        ]
        run_cmd(cmd_refine)

        # æŸ¥æ‰¾ depth_runs ç›®å½•
        candidates = list(refinement_dir.glob("run_*/depth_output/depth_runs"))
        depth_runs_dir = candidates[0] if candidates else (refinement_dir / "depth_output/depth_runs")

        next_pool = gen_dir / "theory_pool"
        avg_delta, count = collect_improved(depth_runs_dir, next_pool)
        print(f"[GEN{gen}] æ”¶é›† {count} ä¸ªæ”¹å†™ç†è®ºï¼Œå¹³å‡æå‡ {avg_delta:.3f}")

        if avg_delta < args.stop_delta:
            print(f"[STOP] å¹³å‡æå‡ {avg_delta:.3f} < stop_delta {args.stop_delta}, æå‰ç»“æŸå¾ªç¯")
            break
        theories_dir_for_next = next_pool
        time.sleep(1)

    print("\nğŸ‰ è¯„ä¼°â†”å¯¹è¯å¾ªç¯å®Œæˆï¼Œå…¨éƒ¨ç»“æœä½äº:", root)


if __name__ == "__main__":
    main() 