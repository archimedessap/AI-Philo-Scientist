#!/usr/bin/env python3
# coding: utf-8
"""
é‡å­ç†è®ºè¯„ä¼°å·¥å…· - å¤šç†è®ºå¤šå®éªŒè¯„ä¼°å™¨
å¯ä»¥æŒ‡å®šLLMæ¨¡å‹ï¼Œè¯„ä¼°å¤šä¸ªé‡å­ç†è®ºå¯¹å¤šä¸ªå®éªŒçš„é¢„æµ‹èƒ½åŠ›

ç”¨æ³•ç¤ºä¾‹:
# è¯„ä¼°å•ä¸ªç†è®ºå¯¹å•ä¸ªå®éªŒ
python demo/demo_1.py --model_source deepseek --model_name deepseek-chat --theory_file demo/theories/theories_more/copenhagen.json --setup_file demo/experiments/c60_double_slit_setup.json --measured_file demo/experiments/c60_double_slit_measured.json --output_file demo/outputs/deepseek_chat/theory_evaluation.json --raw_output_file demo/outputs/deepseek_chat/llm_response_raw.txt

# è¯„ä¼°ç›®å½•ä¸­çš„æ‰€æœ‰ç†è®ºå¯¹å•ä¸ªå®éªŒ
python demo/demo_1.py --model_source deepseek --model_name deepseek-chat --theory_dir demo/theories/theories_more --setup_file demo/experiments/c60_double_slit_setup.json --measured_file demo/experiments/c60_double_slit_measured.json --output_dir demo/outputs/deepseek_chat/theories

# è¯„ä¼°åŒ…å«å¤šä¸ªç†è®ºçš„JSONæ–‡ä»¶ä¸­çš„æ‰€æœ‰ç†è®ºå¯¹å•ä¸ªå®éªŒ
python demo/demo_1.py --model_source deepseek --model_name deepseek-chat --theories_json_file data/synthesized_theories/synthesis_20250520_152448/all_synthesized_theories.json --setup_file demo/experiments/c60_double_slit_setup.json --measured_file demo/experiments/c60_double_slit_measured.json --output_dir demo/outputs/deepseek_chat/theories_batch

# è¯„ä¼°å•ä¸ªç†è®ºå¯¹ç›®å½•ä¸­çš„æ‰€æœ‰å®éªŒ
python demo/demo_1.py --model_source openai --model_name gpt-4o-mini --theory_file demo/theories/theories_more/copenhagen.json --experiment_dir demo/experiments --output_dir demo/outputs/gpt_4o_mini/experiments --use_instrument_correction

# è¯„ä¼°ç›®å½•ä¸­çš„æ‰€æœ‰ç†è®ºå¯¹æ‰€æœ‰å®éªŒ
python demo/demo_1.py --model_source deepseek --model_name deepseek-reasoner --theory_dir demo/theories/theories_more --experiment_dir demo/experiments --output_dir demo/outputs/deepseek_reasoner/all_evaluations --use_instrument_correction

# è¯„ä¼°åŒ…å«å¤šä¸ªç†è®ºçš„JSONæ–‡ä»¶ä¸­çš„æ‰€æœ‰ç†è®ºå¯¹æ‰€æœ‰å®éªŒ
python demo/demo_1.py --model_source deepseek --model_name deepseek-reasoner --theories_json_file data/synthesized_theories/synthesis_20250603_151008/all_synthesized_theories.json --experiment_dir demo/experiments --output_dir demo/outputs/deepseek_reasoner/all_batch_evaluations --use_instrument_correction
"""
import sys, os, json, asyncio, argparse, glob, re
from pathlib import Path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from theory_generation.llm_interface import LLMInterface
from demo.instrument_correction import InstrumentCorrector

async def evaluate_theory_experiment(theory, setup_exp, measured_data, llm, args, output_prefix=None, corrected_setup_exp=None):
    """è¯„ä¼°å•ä¸ªç†è®ºå¯¹å•ä¸ªå®éªŒçš„é¢„æµ‹èƒ½åŠ›"""
    # è·å–å®éªŒID
    exp_id = setup_exp["id"]
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„æµ‹é‡æ•°æ®
    if measured_data and exp_id in measured_data:
        complete_exp = setup_exp.copy()
        complete_exp["measured"] = {
            "value": measured_data[exp_id]["value"],
            "sigma": measured_data[exp_id]["sigma"]
        }
    else:
        print(f"[ERROR] åœ¨æµ‹é‡æ•°æ®ä¸­æ‰¾ä¸åˆ°å®éªŒID: {exp_id}")
        return None
    
    # æå–å®éªŒç›®æ ‡
    exp_type = setup_exp.get("type", setup_exp.get("category", "æœªçŸ¥ç±»å‹"))
    exp_target = setup_exp.get("target_value", setup_exp.get("observable", "æœªå®šä¹‰ç›®æ ‡"))
    
    # ä¿®æ”¹æç¤ºï¼Œæ›´æ¸…æ™°åœ°å±•ç¤ºç†è®ºçš„å„ä¸ªéƒ¨åˆ†ï¼ŒåŒæ—¶ä½¿å…¶é€‚ç”¨äºå„ç§å®éªŒç±»å‹
    # é‡è¦ï¼šåªä½¿ç”¨setup_expï¼ˆä¸åŒ…å«æµ‹é‡å€¼ï¼‰
    prompt = f"""
    You are a quantum-mechanics assistant. Analyze the following experiment using the given theory.
    
    ## Experiment
    {json.dumps(setup_exp, indent=2)}
    
    ## Theory: {theory.get("name", "æœªçŸ¥ç†è®º")}
    
    ### Summary
    {theory.get("summary", "No summary provided.")}
    
    ### Philosophy
    {json.dumps(theory.get("philosophy", {}), indent=2)}
    
    ### Parameters
    {json.dumps(theory.get("parameters", {}), indent=2)}
    
    ### Formalism
    {json.dumps(theory.get("formalism", {}), indent=2)}
    
    ### Semantics
    {json.dumps(theory.get("semantics", {}), indent=2)}
    
    ### Additional Information
    {json.dumps({k: v for k, v in theory.items() if k not in ["name", "summary", "philosophy", "parameters", "formalism", "semantics"]}, indent=2)}
    
    ## Task
    1. Use the theory's principles, formalisms, and concepts to analyze the given experiment.
    2. Derive the predicted value for this experiment ('{exp_target}' for {exp_type} experiment) step by step.
    3. Include detailed mathematical formulas and equations at each step of your derivation.
    4. Make sure to use the semantics and philosophical framework of the theory in your derivation.
    5. Calculate the final numerical value as precisely as possible.
    
    ## Output Format
    Return your answer as TWO JSON objects (one per line):
    {{"derivation": "Step 1: [Formula 1] ... Step 2: [Formula 2] ... etc."}}
    {{"value": 0.XX}}
    
    In your derivation:
    - Use LaTeX notation for all mathematical formulas
    - Show each step of calculation explicitly
    - Explain the physical meaning of each term in your equations
    - Make your mathematical reasoning clear and detailed
    """
    
    # è°ƒç”¨LLM
    theory_name = theory.get("name", "æœªçŸ¥ç†è®º")
    print(f"[INFO] æ­£åœ¨è¯„ä¼°ç†è®º'{theory_name}'å¯¹å®éªŒ'{exp_id}'ï¼Œä½¿ç”¨{args.model_source}/{args.model_name}æ¨¡å‹...")
    response = await llm.query_async([{"role": "user", "content": prompt}], temperature=args.temperature)
    
    # ä¿å­˜å’Œæ‰“å°åŸå§‹å“åº”
    print("\nåŸå§‹LLMå“åº”:\n" + "="*50)
    print(response)
    print("="*50)
    
    # ç¡®å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„
    if args.output_dir:
        # åˆ›å»ºä¸€ä¸ªæœ‰æ„ä¹‰çš„æ–‡ä»¶åå‰ç¼€
        if output_prefix:
            filename_prefix = output_prefix
        else:
            theory_filename = theory_name.replace(" ", "_").lower()
            exp_filename = exp_id.replace(" ", "_").lower()
            filename_prefix = f"{theory_filename}_vs_{exp_filename}"
        
        raw_output_file = os.path.join(args.output_dir, f"{filename_prefix}_response_raw.txt")
        output_file = os.path.join(args.output_dir, f"{filename_prefix}_evaluation.json")
    else:
        raw_output_file = args.raw_output_file
        output_file = args.output_file
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(raw_output_file)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
        print(f"[INFO] å·²åˆ›å»ºç›®å½•: {output_dir}")

    with open(raw_output_file, "w", encoding="utf-8") as f:
        f.write(response)
    print(f"[INFO] åŸå§‹å“åº”å·²ä¿å­˜åˆ°: {raw_output_file}")
    
    # æå–derivationå’Œvalue
    derivation = "æœªæ‰¾åˆ°æ¨å¯¼è¿‡ç¨‹"
    value = None
    
    # è§£æå“åº”ä¸­çš„JSONå¯¹è±¡
    for line in response.splitlines():
        line = line.strip()
        if line.startswith("{") and line.endswith("}"):
            try:
                obj = json.loads(line)
                if "derivation" in obj:
                    derivation = obj["derivation"]
                    print("\næ¨å¯¼è¿‡ç¨‹:\n" + "-"*50)
                    print(derivation)
                    print("-"*50)
                if "value" in obj:
                    value = float(obj["value"])
                    print(f"\né¢„æµ‹å€¼: {value}\n")
            except json.JSONDecodeError:
                continue
    
    # è®¡ç®—ä¸å®éªŒå€¼çš„åå·®ï¼ˆä½¿ç”¨åˆå¹¶åçš„å®Œæ•´å®éªŒæ•°æ®ï¼‰
    chi2 = None
    chi2_corrected = None
    success = None  # æ–°å¢ï¼šé¢„æµ‹æˆåŠŸæ ‡å¿—
    success_corrected = None  # ä»ªå™¨ä¿®æ­£åçš„æˆåŠŸæ ‡å¿—
    chi2_threshold = args.chi2_threshold if hasattr(args, 'chi2_threshold') else 4.0  # é»˜è®¤Ï‡Â²é˜ˆå€¼ä¸º4
    
    # åˆå§‹åŒ–ä»ªå™¨ä¿®æ­£å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    corrector = None
    correction_result = None
    if getattr(args, 'use_instrument_correction', False):
        corrector = InstrumentCorrector()
    
    if value is not None:
        measured = complete_exp.get("measured", {}).get("value")
        sigma = complete_exp.get("measured", {}).get("sigma")
        if measured is not None and sigma is not None:
            # åŸå§‹Ï‡Â²è®¡ç®—ï¼ˆæ— ä»ªå™¨ä¿®æ­£ï¼‰
            chi2 = ((value - measured) / sigma) ** 2
            success = chi2 < chi2_threshold
            
            # ä»ªå™¨ä¿®æ­£è¯„ä¼°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if corrector is not None and corrected_setup_exp is not None:
                # ä½¿ç”¨corrected_setup_expä¸­çš„ä»ªå™¨å‚æ•°è¿›è¡Œä¿®æ­£
                correction_result = corrector.evaluate_with_correction(
                    value, corrected_setup_exp, {"value": measured, "sigma": sigma}
                )
                chi2_corrected = correction_result["chi2_corrected"]
                success_corrected = chi2_corrected < chi2_threshold
                
                print(f"åŸå§‹Ï‡Â²å€¼: {chi2:.4f}")
                print(f"ä¿®æ­£åÏ‡Â²å€¼: {chi2_corrected:.4f}")
                print(f"ç†è®ºé¢„æµ‹å€¼: {value:.4f}")
                print(f"ä»ªå™¨ä¿®æ­£åé¢„æµ‹å€¼: {correction_result['corrected_prediction']:.4f}")
                print(f"å®éªŒæµ‹é‡å€¼: {measured:.4f}")
                print(f"åŸå§‹åå·®: {abs(value - measured):.4f}")
                print(f"ä¿®æ­£ååå·®: {abs(correction_result['corrected_prediction'] - measured):.4f}")
                print(f"åŸå§‹é¢„æµ‹ç»“æœ: {'æˆåŠŸ' if success else 'å¤±è´¥'} (Ï‡Â²é˜ˆå€¼={chi2_threshold})")
                print(f"ä¿®æ­£åé¢„æµ‹ç»“æœ: {'æˆåŠŸ' if success_corrected else 'å¤±è´¥'} (Ï‡Â²é˜ˆå€¼={chi2_threshold})")
                
                # æ˜¾ç¤ºä»ªå™¨å‚æ•°
                if "instrument_corrections" in corrected_setup_exp:
                    inst_params = correction_result["instrument_params"]
                    print(f"ä»ªå™¨å‚æ•°: Î·={inst_params['detection_efficiency']:.3f}, B={inst_params['background_noise']:.3f}, sys={inst_params['systematic_bias']:.3f}")
            else:
                # ä¼ ç»Ÿæ¨¡å¼è¾“å‡º
                print(f"Ï‡Â²å€¼: {chi2:.4f}")
                print(f"ä¸å®éªŒå€¼ {measured} çš„åå·®: {abs(value - measured):.4f}")
                print(f"é¢„æµ‹ç»“æœ: {'æˆåŠŸ' if success else 'å¤±è´¥'} (Ï‡Â²é˜ˆå€¼={chi2_threshold})")
    
    # åˆ›å»ºåŒ…å«æ¨å¯¼å’Œç»“æœçš„ç»“æ„åŒ–è¾“å‡º
    structured_output = {
        "theory_name": theory_name,
        "experiment_id": exp_id,
        "derivation": derivation,
        "predicted_value": float(value),  # ç¡®ä¿æ˜¯PythonåŸç”Ÿfloat
        "measured_value": float(measured_data[exp_id]["value"]),
        "sigma": float(measured_data[exp_id]["sigma"]),
        "chi2": float(chi2),
        "success": bool(success),  # ç¡®ä¿æ˜¯PythonåŸç”Ÿbool
        "chi2_threshold": float(chi2_threshold),
        "model_info": {
            "source": args.model_source,
            "name": args.model_name,
            "temperature": float(args.temperature)
        }
    }
    
    # å¦‚æœä½¿ç”¨äº†ä»ªå™¨ä¿®æ­£ï¼Œæ·»åŠ ç›¸å…³ä¿¡æ¯
    if correction_result is not None:
        structured_output.update({
            "corrected_predicted_value": float(correction_result["corrected_prediction"]),
            "total_sigma": float(correction_result["total_sigma"]),
            "chi2_corrected": float(chi2_corrected),
            "success_corrected": bool(success_corrected),  # ç¡®ä¿æ˜¯PythonåŸç”Ÿbool
            "instrument_correction": correction_result["instrument_params"]
        })
    
    # ä¿å­˜ç»“æ„åŒ–è¾“å‡º
    output_dir = os.path.dirname(output_file)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
        print(f"[INFO] å·²åˆ›å»ºç›®å½•: {output_dir}")

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(structured_output, f, ensure_ascii=False, indent=2)
    print(f"\n[INFO] è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    return structured_output

def load_experiments_from_directory(experiment_dir, use_instrument_correction=False):
    """ä»ç›®å½•ä¸­åŠ è½½æ‰€æœ‰å®éªŒæ–‡ä»¶ï¼ŒåŒ¹é…è®¾ç½®å’Œæµ‹é‡æ•°æ®"""
    print(f"[INFO] æ­£åœ¨ä»ç›®å½• {experiment_dir} åŠ è½½å®éªŒ...")
    
    # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
    experiment_files = glob.glob(os.path.join(experiment_dir, "*.json"))
    
    # åˆ†ç¦»è®¾ç½®å’Œæµ‹é‡æ–‡ä»¶
    setup_files = {}
    setup_corrected_files = {}  # æ–°å¢ï¼šå­˜å‚¨ä¿®æ­£ç‰ˆæœ¬çš„æ–‡ä»¶
    measured_files = {}
    
    for file_path in experiment_files:
        filename = os.path.basename(file_path)
        # é€šè¿‡æ–‡ä»¶ååˆ¤æ–­ç±»å‹
        if "setup_corrected" in filename:
            # setup_corrected.json æ–‡ä»¶
            match = re.match(r"(.+?)_setup_corrected\.json", filename)
            if match:
                exp_id = match.group(1)
                setup_corrected_files[exp_id] = file_path
        elif "setup" in filename and filename.endswith("_setup.json"):
            # åŸå§‹ setup.json æ–‡ä»¶
            match = re.match(r"(.+?)_setup\.json", filename)
            if match:
                exp_id = match.group(1)
                setup_files[exp_id] = file_path
        elif "measured" in filename:
            # æµ‹é‡æ•°æ®æ–‡ä»¶
            match = re.match(r"(.+?)_measured\.json", filename)
            if match:
                exp_id = match.group(1)
                measured_files[exp_id] = file_path
    
    # æ ¹æ®æ˜¯å¦ä½¿ç”¨ä»ªå™¨ä¿®æ­£é€‰æ‹©åŠ è½½ç­–ç•¥
    if use_instrument_correction:
        # ä»ªå™¨ä¿®æ­£æ¨¡å¼ï¼šéœ€è¦åŒæ—¶æœ‰åŸå§‹setupæ–‡ä»¶å’Œcorrectedæ–‡ä»¶
        common_exp_ids = set(setup_files.keys()) & set(setup_corrected_files.keys()) & set(measured_files.keys())
        print(f"[INFO] ä»ªå™¨ä¿®æ­£æ¨¡å¼ï¼šæ‰¾åˆ° {len(common_exp_ids)} ä¸ªå®Œæ•´å®éªŒæ•°æ®é›†")
    else:
        # éä¿®æ­£æ¨¡å¼ï¼šä¼˜å…ˆä½¿ç”¨correctedæ–‡ä»¶ï¼ˆè®©LLMè‡ªå·±å¤„ç†ï¼‰ï¼Œå›é€€åˆ°åŸå§‹æ–‡ä»¶
        available_setup_ids = set(setup_corrected_files.keys()) | set(setup_files.keys())
        common_exp_ids = available_setup_ids & set(measured_files.keys())
        print(f"[INFO] æ ‡å‡†æ¨¡å¼ï¼šæ‰¾åˆ° {len(common_exp_ids)} ä¸ªå®Œæ•´å®éªŒæ•°æ®é›†")
    
    # åŠ è½½å®éªŒæ•°æ®
    experiments = {}
    experiments_corrected = {}  # æ–°å¢ï¼šå­˜å‚¨ä¿®æ­£ç‰ˆæœ¬çš„å®éªŒè®¾ç½®
    measured_db = {}
    
    for exp_id in common_exp_ids:
        try:
            # åŠ è½½æµ‹é‡æ•°æ®
            with open(measured_files[exp_id], 'r') as f:
                measured_data = json.load(f)
            measured_db[exp_id] = measured_data
            
            if use_instrument_correction:
                # ä»ªå™¨ä¿®æ­£æ¨¡å¼ï¼šåˆ†åˆ«åŠ è½½åŸå§‹å’Œä¿®æ­£ç‰ˆæœ¬
                with open(setup_files[exp_id], 'r') as f:
                    setup_data = json.load(f)
                with open(setup_corrected_files[exp_id], 'r') as f:
                    setup_corrected_data = json.load(f)
                
                experiments[exp_id] = setup_data  # ç”¨äºLLMçº¯ç†è®ºé¢„æµ‹
                experiments_corrected[exp_id] = setup_corrected_data  # ç”¨äºä»ªå™¨ä¿®æ­£
                
                print(f"[INFO] å·²åŠ è½½å®éªŒ: {exp_id} (åŸå§‹+ä¿®æ­£è®¾ç½®)")
            else:
                # éä¿®æ­£æ¨¡å¼ï¼šä¼˜å…ˆä½¿ç”¨correctedï¼Œå›é€€åˆ°åŸå§‹
                if exp_id in setup_corrected_files:
                    with open(setup_corrected_files[exp_id], 'r') as f:
                        setup_data = json.load(f)
                    file_type = "corrected"
                else:
                    with open(setup_files[exp_id], 'r') as f:
                        setup_data = json.load(f)
                    file_type = "original"
                
                experiments[exp_id] = setup_data
                print(f"[INFO] å·²åŠ è½½å®éªŒ: {exp_id} (ä½¿ç”¨{file_type}è®¾ç½®)")
                
        except Exception as e:
            print(f"[ERROR] åŠ è½½å®éªŒ {exp_id} æ—¶å‡ºé”™: {str(e)}")
    
    return experiments, experiments_corrected, measured_db

async def main():
    # å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description="é‡å­ç†è®ºè¯„ä¼°å·¥å…·")
    
    # ç†è®ºç›¸å…³å‚æ•°
    theory_group = parser.add_mutually_exclusive_group(required=True)
    theory_group.add_argument("--theory_file", 
                      help="å•ä¸ªç†è®ºJSONæ–‡ä»¶è·¯å¾„")
    theory_group.add_argument("--theory_dir", 
                      help="åŒ…å«å¤šä¸ªç†è®ºJSONæ–‡ä»¶çš„ç›®å½•è·¯å¾„")
    theory_group.add_argument("--theories_json_file",
                      help="åŒ…å«å¤šä¸ªç†è®ºçš„å•ä¸ªJSONæ–‡ä»¶è·¯å¾„ï¼ˆç†è®ºæ•°ç»„ï¼‰")
    
    # å®éªŒç›¸å…³å‚æ•°
    experiment_group = parser.add_mutually_exclusive_group(required=True)
    experiment_group.add_argument("--setup_file",
                      help="å®éªŒè®¾ç½®JSONæ–‡ä»¶è·¯å¾„")
    experiment_group.add_argument("--experiment_dir",
                      help="åŒ…å«å¤šä¸ªå®éªŒçš„ç›®å½•è·¯å¾„ï¼ˆå«setupå’Œmeasuredæ–‡ä»¶ï¼‰")
    
    parser.add_argument("--measured_file",
                      help="å®éªŒæµ‹é‡å€¼JSONæ–‡ä»¶è·¯å¾„ï¼ˆä¸--setup_fileä¸€èµ·ä½¿ç”¨ï¼‰")
    
    parser.add_argument("--model_source", default="deepseek",
                      choices=["openai", "deepseek"],
                      help="LLMæ¨¡å‹æ¥æº")
    parser.add_argument("--model_name", default="deepseek-chat",
                      help="LLMæ¨¡å‹åç§°")
    parser.add_argument("--temperature", type=float, default=0.2,
                      help="LLMæ¸©åº¦å‚æ•°")
    parser.add_argument("--chi2_threshold", type=float, default=10.0,
                      help="ç†è®ºé¢„æµ‹æˆåŠŸçš„Ï‡Â²é˜ˆå€¼")
    parser.add_argument("--use_instrument_correction", action="store_true",
                      help="æ˜¯å¦ä½¿ç”¨ä»ªå™¨ä¿®æ­£ï¼ˆé»˜è®¤å…³é—­ï¼Œå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰")
    parser.add_argument("--correction_mode", choices=["raw", "corrected", "both"], default="both",
                      help="è¯„ä¼°æ¨¡å¼ï¼šraw(ä»…åŸå§‹)ï¼Œcorrected(ä»…ä¿®æ­£)ï¼Œboth(ä¸¤è€…)")
    
    # è¾“å‡ºç›¸å…³å‚æ•°
    output_group = parser.add_mutually_exclusive_group(required=True)
    output_group.add_argument("--output_file", 
                      help="å•ä¸ªè¯„ä¼°ç»“æœçš„JSONæ–‡ä»¶è·¯å¾„")
    output_group.add_argument("--output_dir", 
                      help="å¤šä¸ªè¯„ä¼°ç»“æœçš„è¾“å‡ºç›®å½•")
    
    parser.add_argument("--raw_output_file", 
                      help="åŸå§‹LLMå“åº”æ–‡æœ¬æ–‡ä»¶è·¯å¾„ï¼ˆä»…ç”¨äºå•ä¸ªè¯„ä¼°ï¼‰")
    
    args = parser.parse_args()
    
    # æ ¡éªŒå‚æ•°ç»„åˆ
    if args.setup_file and not args.measured_file:
        parser.error("ä½¿ç”¨--setup_fileæ—¶å¿…é¡»æŒ‡å®š--measured_file")
    if (args.theory_file and args.setup_file) and not (args.output_file and args.raw_output_file):
        parser.error("è¯„ä¼°å•ä¸ªç†è®ºå¯¹å•ä¸ªå®éªŒæ—¶ï¼Œå¿…é¡»æŒ‡å®š--output_fileå’Œ--raw_output_file")
    if (args.theory_dir or args.theories_json_file or args.experiment_dir) and not args.output_dir:
        parser.error("åœ¨æ‰¹é‡è¯„ä¼°æ¨¡å¼ä¸‹å¿…é¡»æŒ‡å®š--output_dir")
    
    # åˆ›å»ºLLM
    llm = LLMInterface(model_name=args.model_name, model_source=args.model_source)
    
    # åŠ è½½å®éªŒæ•°æ®
    if args.experiment_dir:
        # ä»ç›®å½•åŠ è½½å¤šä¸ªå®éªŒ
        experiments, experiments_corrected, measured_db = load_experiments_from_directory(args.experiment_dir, args.use_instrument_correction)
        if not experiments:
            parser.error(f"åœ¨ç›®å½• {args.experiment_dir} ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„å®éªŒæ•°æ®")
    else:
        # åŠ è½½å•ä¸ªå®éªŒ
        with open(args.setup_file) as f:
            setup_exp = json.load(f)
        
        with open(args.measured_file) as f:
            measured_data = json.load(f)
        
        # å°†æµ‹é‡å€¼è½¬æ¢ä¸ºIDç´¢å¼•çš„å­—å…¸
        if isinstance(measured_data, list):
            measured_db = {d["id"]: d for d in measured_data}
        else:
            measured_db = {measured_data["id"]: measured_data}
        
        experiments = {setup_exp["id"]: setup_exp}
        experiments_corrected = {}  # å•ä¸ªå®éªŒæ¨¡å¼ä¸‹ä¸æ”¯æŒä»ªå™¨ä¿®æ­£
    
    # åŠ è½½ç†è®º
    if args.theory_dir:
        # ä»ç›®å½•åŠ è½½å¤šä¸ªç†è®º
        theory_files = glob.glob(os.path.join(args.theory_dir, "*.json"))
        print(f"[INFO] åœ¨ç›®å½• {args.theory_dir} ä¸­æ‰¾åˆ° {len(theory_files)} ä¸ªç†è®ºæ–‡ä»¶")
        theories = {}
        
        for theory_file in theory_files:
            try:
                with open(theory_file) as f:
                    theory = json.load(f)
                theory_name = theory.get("name", os.path.basename(theory_file))
                theories[theory_name] = theory
            except Exception as e:
                print(f"[ERROR] åŠ è½½ç†è®ºæ–‡ä»¶ {theory_file} æ—¶å‡ºé”™: {str(e)}")
    elif args.theories_json_file:
        # ä»å•ä¸ªJSONæ–‡ä»¶åŠ è½½å¤šä¸ªç†è®ºï¼ˆç†è®ºæ•°ç»„ï¼‰
        print(f"[INFO] ä» {args.theories_json_file} åŠ è½½å¤šä¸ªç†è®º")
        theories = {}
        try:
            with open(args.theories_json_file) as f:
                theories_list = json.load(f)
            
            if not isinstance(theories_list, list):
                parser.error(f"æ–‡ä»¶ {args.theories_json_file} ä¸åŒ…å«ç†è®ºæ•°ç»„")
            
            print(f"[INFO] åœ¨æ–‡ä»¶ä¸­æ‰¾åˆ° {len(theories_list)} ä¸ªç†è®º")
            for i, theory in enumerate(theories_list):
                theory_name = theory.get("name", f"ç†è®º_{i+1}")
                theories[theory_name] = theory
        except Exception as e:
            parser.error(f"åŠ è½½ç†è®ºæ–‡ä»¶ {args.theories_json_file} æ—¶å‡ºé”™: {str(e)}")
    else:
        # åŠ è½½å•ä¸ªç†è®º
        with open(args.theory_file) as f:
            theory = json.load(f)
        theory_name = theory.get("name", "æœªçŸ¥ç†è®º")
        theories = {theory_name: theory}
    
    # è¯„ä¼°æ‰€æœ‰ç†è®ºå¯¹æ‰€æœ‰å®éªŒ
    all_results = []
    
    for theory_name, theory in theories.items():
        theory_results = []
        for exp_id, setup_exp in experiments.items():
            try:
                # åˆ›å»ºä¸€ä¸ªæœ‰æ„ä¹‰çš„è¾“å‡ºå‰ç¼€
                theory_filename = theory_name.replace(" ", "_").lower()
                exp_filename = exp_id.replace(" ", "_").lower()
                output_prefix = f"{theory_filename}_vs_{exp_filename}"
                
                # è¯„ä¼°å•ä¸ªç†è®ºå¯¹å•ä¸ªå®éªŒ
                result = await evaluate_theory_experiment(
                    theory, setup_exp, measured_db, llm, args, output_prefix, experiments_corrected.get(exp_id)
                )
                
                if result:
                    theory_results.append(result)
                    all_results.append(result)
            except Exception as e:
                print(f"[ERROR] è¯„ä¼°ç†è®º '{theory_name}' å¯¹å®éªŒ '{exp_id}' æ—¶å‡ºé”™: {str(e)}")
        
        # ä¿å­˜æ¯ä¸ªç†è®ºçš„æ±‡æ€»ç»“æœ
        if args.output_dir and len(theory_results) > 0:
            theory_summary_file = os.path.join(
                args.output_dir, 
                f"{theory_name.replace(' ', '_').lower()}_summary.json"
            )
            
            with open(theory_summary_file, "w", encoding="utf-8") as f:
                json.dump(theory_results, f, ensure_ascii=False, indent=2)
            print(f"[INFO] ç†è®º '{theory_name}' çš„æ±‡æ€»è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {theory_summary_file}")
    
    # ä¿å­˜æ€»ä½“æ±‡æ€»ç»“æœ
    if args.output_dir and len(all_results) > 0:
        # åˆ›å»ºå®éªŒæ±‡æ€»ç»“æœ
        experiment_results = {}
        for result in all_results:
            exp_id = result["experiment_id"]
            if exp_id not in experiment_results:
                experiment_results[exp_id] = []
            experiment_results[exp_id].append({
                "theory_name": result["theory_name"],
                "predicted_value": result["predicted_value"],
                "chi2": result["chi2"],
                "success": result["success"]  # æ·»åŠ æˆåŠŸæ ‡å¿—
            })
        
        # ä¿å­˜æ¯ä¸ªå®éªŒçš„æ±‡æ€»ç»“æœ
        for exp_id, results in experiment_results.items():
            exp_summary_file = os.path.join(
                args.output_dir,
                f"{exp_id.replace(' ', '_').lower()}_comparison.json"
            )
            with open(exp_summary_file, "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"[INFO] å®éªŒ '{exp_id}' çš„ç†è®ºæ¯”è¾ƒç»“æœå·²ä¿å­˜åˆ°: {exp_summary_file}")
        
        # ä¿å­˜æ€»ä½“æ±‡æ€»
        summary_file = os.path.join(args.output_dir, "evaluation_summary.json")
        with open(summary_file, "w", encoding="utf-8") as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        print(f"[INFO] æ‰€æœ‰è¯„ä¼°ç»“æœçš„æ±‡æ€»å·²ä¿å­˜åˆ°: {summary_file}")
        
        # ç”Ÿæˆç†è®ºæ’åæ¯”è¾ƒ
        theory_scores = {}
        theory_success_rates = {}  # æ–°å¢ï¼šè®°å½•æ¯ä¸ªç†è®ºçš„æˆåŠŸç‡
        
        for result in all_results:
            theory_name = result["theory_name"]
            chi2 = result.get("chi2")
            success = result.get("success")
            
            # å½“ä½¿ç”¨ä»ªå™¨ä¿®æ­£æ—¶ï¼Œä¼˜å…ˆä½¿ç”¨ä¿®æ­£åçš„ç»“æœ
            if args.use_instrument_correction and "success_corrected" in result:
                chi2 = result.get("chi2_corrected", chi2)
                success = result.get("success_corrected", success)
            
            # è®°å½•Ï‡Â²å¾—åˆ†
            if chi2 is not None:
                if theory_name not in theory_scores:
                    theory_scores[theory_name] = []
                theory_scores[theory_name].append(chi2)
            
            # è®°å½•æˆåŠŸ/å¤±è´¥
            if success is not None:
                if theory_name not in theory_success_rates:
                    theory_success_rates[theory_name] = {"success": 0, "total": 0}
                theory_success_rates[theory_name]["total"] += 1
                if success:
                    theory_success_rates[theory_name]["success"] += 1
        
        # è®¡ç®—å¹³å‡Ï‡Â²å¾—åˆ†å’ŒæˆåŠŸç‡
        theory_rankings = []
        for theory_name, scores in theory_scores.items():
            avg_chi2 = sum(scores) / len(scores) if scores else float('inf')
            
            # è®¡ç®—æˆåŠŸç‡
            success_rate = 0
            if theory_name in theory_success_rates and theory_success_rates[theory_name]["total"] > 0:
                success_rate = theory_success_rates[theory_name]["success"] / theory_success_rates[theory_name]["total"]
            
            theory_rankings.append({
                "theory_name": theory_name,
                "experiments_count": len(scores),
                "average_chi2": avg_chi2,
                "total_chi2": sum(scores) if scores else float('inf'),
                "success_count": theory_success_rates.get(theory_name, {}).get("success", 0),
                "success_rate": success_rate,
                "chi2_threshold": args.chi2_threshold
            })
        
        # æŒ‰æˆåŠŸç‡æ’åºï¼ˆä»é«˜åˆ°ä½ï¼‰ï¼Œç„¶åæŒ‰å¹³å‡Ï‡Â²æ’åºï¼ˆä»ä½åˆ°é«˜ï¼‰
        theory_rankings.sort(key=lambda x: (-x["success_rate"], x["average_chi2"]))
        
        # ä¿å­˜ç†è®ºæ’å
        ranking_file = os.path.join(args.output_dir, "theory_rankings.json")
        with open(ranking_file, "w", encoding="utf-8") as f:
            json.dump(theory_rankings, f, ensure_ascii=False, indent=2)
        print(f"[INFO] ç†è®ºæ’åç»“æœå·²ä¿å­˜åˆ°: {ranking_file}")
        
        # æ‰“å°æ’åæ€»ç»“
        print("\nç†è®ºæ’åæ€»ç»“:")
        print("-" * 80)
        print(f"{'ç†è®ºåç§°':<30} {'æˆåŠŸç‡':<10} {'å¹³å‡Ï‡Â²':<10} {'å®éªŒæ•°':<10}")
        print("-" * 80)
        for rank in theory_rankings:
            print(f"{rank['theory_name']:<30} {rank['success_rate']*100:.1f}% {rank['average_chi2']:<10.4f} {rank['experiments_count']:<10}")
        print("-" * 80)
        
        # è‡ªåŠ¨è§’è‰²è¯„ä¼°ï¼šå¯¹æˆåŠŸç‡>=80%çš„ç†è®ºè¿›è¡Œå¤šè§’è‰²è¯„ä¼°
        high_success_theories = [
            rank for rank in theory_rankings 
            if rank['success_rate'] >= 0.8  # 80%æˆåŠŸç‡é˜ˆå€¼
        ]
        
        if high_success_theories:
            print(f"\n[INFO] å‘ç° {len(high_success_theories)} ä¸ªæˆåŠŸç‡â‰¥80%çš„ç†è®ºï¼Œå¼€å§‹è§’è‰²è¯„ä¼°...")
            
            # å¯¼å…¥è§’è‰²è¯„ä¼°æ¨¡å—
            try:
                import sys
                sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
                from theory_validation.agent_validation.theory_evaluator import TheoryEvaluator
                
                # åˆå§‹åŒ–è§’è‰²è¯„ä¼°å™¨
                role_evaluator = TheoryEvaluator(llm)
                
                # åˆ›å»ºè§’è‰²è¯„ä¼°è¾“å‡ºç›®å½•
                role_output_dir = os.path.join(args.output_dir, "role_evaluations")
                os.makedirs(role_output_dir, exist_ok=True)
                
                role_results = []
                
                for rank in high_success_theories:
                    theory_name = rank['theory_name']
                    theory_data = theories[theory_name]
                    
                    print(f"[INFO] æ­£åœ¨å¯¹ç†è®º '{theory_name}' è¿›è¡Œè§’è‰²è¯„ä¼°...")
                    
                    try:
                        # æ‰§è¡Œè§’è‰²è¯„ä¼°
                        role_result = await role_evaluator.evaluate_theory(theory_data, predictor_module=None)
                        role_result['experiment_success_rate'] = rank['success_rate']
                        role_result['average_chi2'] = rank['average_chi2']
                        role_results.append(role_result)
                        
                        # ä¿å­˜å•ä¸ªç†è®ºçš„è§’è‰²è¯„ä¼°ç»“æœ
                        theory_role_file = os.path.join(
                            role_output_dir,
                            f"{theory_name.replace(' ', '_').lower()}_role_evaluation.json"
                        )
                        with open(theory_role_file, "w", encoding="utf-8") as f:
                            json.dump(role_result, f, ensure_ascii=False, indent=2)
                        
                        print(f"[INFO] ç†è®º '{theory_name}' çš„è§’è‰²è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {theory_role_file}")
                        
                    except Exception as e:
                        print(f"[ERROR] è¯„ä¼°ç†è®º '{theory_name}' çš„è§’è‰²æ—¶å‡ºé”™: {str(e)}")
                
                # ä¿å­˜ç»¼åˆè§’è‰²è¯„ä¼°ç»“æœ
                if role_results:
                    role_summary_file = os.path.join(role_output_dir, "role_evaluation_summary.json")
                    with open(role_summary_file, "w", encoding="utf-8") as f:
                        json.dump(role_results, f, ensure_ascii=False, indent=2)
                    
                    # è®¡ç®—ç»¼åˆæ’åï¼ˆå®éªŒæˆåŠŸç‡ + è§’è‰²è¯„ä¼°ï¼‰
                    combined_rankings = []
                    for role_result in role_results:
                        theory_name = role_result['theory']['name']
                        
                        # è®¡ç®—è§’è‰²è¯„ä¼°å¹³å‡åˆ†
                        role_scores = [
                            eval_result['overall_score'] 
                            for eval_result in role_result['evaluations']
                            if 'overall_score' in eval_result and eval_result['overall_score'] is not None
                        ]
                        avg_role_score = sum(role_scores) / len(role_scores) if role_scores else 0
                        
                        # ç»¼åˆè¯„åˆ† = å®éªŒæˆåŠŸç‡ * 0.6 + è§’è‰²è¯„ä¼°åˆ† * 0.4
                        combined_score = (
                            role_result['experiment_success_rate'] * 0.6 + 
                            avg_role_score / 10.0 * 0.4  # è§’è‰²è¯„ä¼°åˆ†é€šå¸¸æ˜¯1-10åˆ†ï¼Œå½’ä¸€åŒ–åˆ°0-1
                        )
                        
                        combined_rankings.append({
                            'theory_name': theory_name,
                            'experiment_success_rate': role_result['experiment_success_rate'],
                            'average_chi2': role_result['average_chi2'],
                            'average_role_score': avg_role_score,
                            'combined_score': combined_score,
                            'role_details': {
                                eval_result['role']: eval_result.get('overall_score', 0)
                                for eval_result in role_result['evaluations']
                            }
                        })
                    
                    # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
                    combined_rankings.sort(key=lambda x: -x['combined_score'])
                    
                    # ä¿å­˜ç»¼åˆæ’å
                    combined_ranking_file = os.path.join(role_output_dir, "combined_rankings.json")
                    with open(combined_ranking_file, "w", encoding="utf-8") as f:
                        json.dump(combined_rankings, f, ensure_ascii=False, indent=2)
                    
                    print(f"[INFO] è§’è‰²è¯„ä¼°æ±‡æ€»å·²ä¿å­˜åˆ°: {role_summary_file}")
                    print(f"[INFO] ç»¼åˆæ’åå·²ä¿å­˜åˆ°: {combined_ranking_file}")
                    
                    # æ‰“å°ç»¼åˆæ’å
                    print("\nğŸ† ç»¼åˆæ’åï¼ˆå®éªŒ 60% + è§’è‰²è¯„ä¼° 40%ï¼‰:")
                    print("-" * 100)
                    print(f"{'ç†è®ºåç§°':<30} {'å®éªŒæˆåŠŸç‡':<12} {'è§’è‰²å¹³å‡åˆ†':<12} {'ç»¼åˆè¯„åˆ†':<12} {'è¯¦ç»†è¯„åˆ†'}")
                    print("-" * 100)
                    for rank in combined_rankings:
                        role_detail = ", ".join([f"{role}:{score:.1f}" for role, score in rank['role_details'].items()])
                        print(f"{rank['theory_name']:<30} {rank['experiment_success_rate']*100:>8.1f}% "
                              f"{rank['average_role_score']:>10.1f} {rank['combined_score']:>10.3f} {role_detail}")
                    print("-" * 100)
                    
                else:
                    print("[WARNING] æ²¡æœ‰æˆåŠŸå®Œæˆçš„è§’è‰²è¯„ä¼°ç»“æœ")
                    
            except ImportError as e:
                print(f"[ERROR] æ— æ³•å¯¼å…¥è§’è‰²è¯„ä¼°æ¨¡å—: {str(e)}")
                print("[INFO] è·³è¿‡è§’è‰²è¯„ä¼°æ­¥éª¤")
            except Exception as e:
                print(f"[ERROR] è§’è‰²è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        else:
            print("\n[INFO] æ²¡æœ‰ç†è®ºè¾¾åˆ°80%æˆåŠŸç‡é˜ˆå€¼ï¼Œè·³è¿‡è§’è‰²è¯„ä¼°")

if __name__ == "__main__":
    asyncio.run(main())