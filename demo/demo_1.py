#!/usr/bin/env python3
# coding: utf-8
"""
é‡å­ç†è®ºè¯„ä¼°å·¥å…· - å¤šç†è®ºå¤šå®éªŒè¯„ä¼°å™¨
å¯ä»¥æŒ‡å®šLLMæ¨¡å‹ï¼Œè¯„ä¼°å¤šä¸ªé‡å­ç†è®ºå¯¹å¤šä¸ªå®éªŒçš„é¢„æµ‹èƒ½åŠ›

"""
import sys, os, json, asyncio, argparse, glob, re, time
from pathlib import Path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from theory_generation.llm_interface import LLMInterface
from demo.instrument_correction import InstrumentCorrector
# å¯¼å…¥æ–°çš„è§’è‰²è¯„ä¼°æ¨¡å—
from demo.auto_role_evaluation import run_role_evaluation_for_theories

def load_theories_from_sources(theories_path: str, schema_version: str = "2.1") -> dict:
    """
    ä»ç›®å½•æˆ–å•ä¸ªæ–‡ä»¶åŠ è½½ç†è®ºæ•°æ®
    
    Args:
        theories_path: ç†è®ºç›®å½•æˆ–å•ä¸ªç†è®ºæ–‡ä»¶çš„è·¯å¾„
        schema_version: è¦æ±‚åŠ è½½çš„ç†è®ºschemaç‰ˆæœ¬ ('any' to load all)
        
    Returns:
        dict: ç†è®ºåç§°åˆ°ç†è®ºæ•°æ®çš„æ˜ å°„
    """
    theories = {}
    
    # ç¡®å®šæ˜¯ç›®å½•è¿˜æ˜¯æ–‡ä»¶
    if os.path.isdir(theories_path):
        theory_files = glob.glob(os.path.join(theories_path, "*.json"))
        print(f"[INFO] åœ¨ç›®å½• {theories_path} ä¸­æ‰¾åˆ° {len(theory_files)} ä¸ªç†è®ºæ–‡ä»¶")
    elif os.path.isfile(theories_path) and theories_path.endswith('.json'):
        theory_files = [theories_path]
        print(f"[INFO] æ­£åœ¨åŠ è½½å•ä¸ªç†è®ºæ–‡ä»¶: {theories_path}")
    else:
        print(f"[ERROR] æ— æ•ˆçš„ç†è®ºè·¯å¾„: {theories_path}")
        return {}
        
    for theory_file in theory_files:
        try:
            with open(theory_file, 'r', encoding='utf-8') as f:
                theory = json.load(f)
            
            # å…¼å®¹åŒ…å«ç†è®ºåˆ—è¡¨çš„JSONæ–‡ä»¶
            if isinstance(theory, list):
                theory_list = theory
            else:
                theory_list = [theory]
            
            for t in theory_list:
                # æ£€æŸ¥schemaç‰ˆæœ¬
                load_any_schema = schema_version is None or schema_version.lower() == 'any'
                file_schema_version = t.get("metadata", {}).get("schema_version")
                
                if not load_any_schema and file_schema_version != schema_version:
                    print(f"[WARN] è·³è¿‡ç†è®º '{t.get('name', 'æœªå‘½å')}': schemaç‰ˆæœ¬ä¸åŒ¹é… (éœ€è¦ {schema_version}, æ–‡ä»¶ä¸º {file_schema_version})")
                    continue

                theory_name = t.get("name", os.path.basename(theory_file))
                if theory_name in theories:
                    print(f"[WARN] å‘ç°é‡å¤çš„ç†è®ºåç§° '{theory_name}'ï¼Œå°†è¦†ç›–æ—§ç‰ˆæœ¬ã€‚")
                theories[theory_name] = t

        except Exception as e:
            print(f"[ERROR] åŠ è½½ç†è®ºæ–‡ä»¶ {theory_file} æ—¶å‡ºé”™: {str(e)}")
            
    return theories

async def evaluate_theory_experiment(theory, setup_exp, measured_data, llm, args, output_prefix=None, corrected_setup_exp=None):
    """è¯„ä¼°å•ä¸ªç†è®ºå¯¹å•ä¸ªå®éªŒçš„é¢„æµ‹èƒ½åŠ›"""
    # è·å–å®éªŒID
    exp_id = setup_exp["id"]
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„æµ‹é‡æ•°æ®
    if measured_data and exp_id in measured_data:
        complete_exp = setup_exp.copy()
        complete_exp["measured"] = {
            "value": measured_data[exp_id]["value"],
            "sigma": measured_data[exp_id]["sigma"]
        }
    else:
        print(f"[ERROR] åœ¨æµ‹é‡æ•°æ®ä¸­æ‰¾ä¸åˆ°å®éªŒID: {exp_id}")
        return None
    
    # æå–å®éªŒç›®æ ‡
    exp_type = setup_exp.get("type", setup_exp.get("category", "æœªçŸ¥ç±»å‹"))
    exp_target = setup_exp.get("target_value", setup_exp.get("observable", "æœªå®šä¹‰ç›®æ ‡"))
    
    # ä½¿ç”¨Schema v2.1æ›´æ–°Prompt
    prompt = f"""
    You are a quantum physics expert tasked with evaluating a theoretical model against experimental data.
    
    ## Theory: {theory.get("name", "Unknown Theory")} (Schema Version: {theory.get("metadata", {}).get("schema_version", "N/A")})
    
    ### Core Identity
    - **UID:** {theory.get("metadata", {}).get("uid", "N/A")}
    - **Lineage:** {json.dumps(theory.get("metadata", {}).get("lineage", {}), indent=2)}
    - **Relation to SQM:** {theory.get("mathematical_relation_to_sqm", "Not specified")}

    ### Core Principles
    {json.dumps(theory.get("core_principles", "No principles provided."), indent=2)}
    
    ### Formalism
    {json.dumps(theory.get("formalism", "No formalism provided."), indent=2)}
    
    ### Predictions and Verifiability
    {json.dumps(theory.get("predictions_and_verifiability", "No predictions provided."), indent=2)}
    
    ## Experiment
    {json.dumps(setup_exp, indent=2)}
    
    ## Task
    1.  **Analyze**: Based on the theory's **Core Principles** and **Formalism**, analyze the provided experiment.
    2.  **Derive**: Step-by-step, derive the predicted value for the experiment's target: **'{exp_target}'**. 
        - If the theory is an **interpretation** of SQM, use standard quantum formalism and explain how the theory's principles interpret the result.
        - If the theory is a **modification** or **extension** of SQM, explicitly use the modified equations from the **Formalism** section. Highlight how the `deviations_from_sqm` lead to a different prediction.
    3.  **Calculate**: Compute the final numerical value for the prediction.
    4.  **Justify**: Explain how the derivation is a logical consequence of the theory's specific axioms and mathematical structure.

    ## Output Format
    Return your answer as TWO JSON objects (one per line):
    {{"derivation": "Step-by-step derivation using LaTeX for math, explaining each step logically from the theory's perspective."}}
    {{"value": <your_calculated_numerical_value>}}
    """
    
    # è°ƒç”¨LLM
    theory_name = theory.get("name", "æœªçŸ¥ç†è®º")
    print(f"[INFO] æ­£åœ¨è¯„ä¼°ç†è®º'{theory_name}'å¯¹å®éªŒ'{exp_id}'ï¼Œä½¿ç”¨{args.model_source}/{args.model_name}æ¨¡å‹...")
    response = await llm.query_async([{"role": "user", "content": prompt}], temperature=args.temperature)
    
    # ä¿å­˜å’Œæ‰“å°åŸå§‹å“åº”
    print("\nåŸå§‹LLMå“åº”:\n" + "="*50)
    print(response)
    print("="*50)
    
    # ç¡®å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„
    if args.output_dir:
        # åˆ›å»ºä¸€ä¸ªæœ‰æ„ä¹‰çš„æ–‡ä»¶åå‰ç¼€
        if output_prefix:
            filename_prefix = output_prefix
        else:
            theory_filename = theory_name.replace(" ", "_").lower()
            exp_filename = exp_id.replace(" ", "_").lower()
            filename_prefix = f"{theory_filename}_vs_{exp_filename}"
        
        raw_output_file = os.path.join(args.output_dir, f"{filename_prefix}_response_raw.txt")
        output_file = os.path.join(args.output_dir, f"{filename_prefix}_evaluation.json")
    else:
        raw_output_file = args.raw_output_file
        output_file = args.output_file
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(raw_output_file)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
        print(f"[INFO] å·²åˆ›å»ºç›®å½•: {output_dir}")

    with open(raw_output_file, "w", encoding="utf-8") as f:
        f.write(response)
    print(f"[INFO] åŸå§‹å“åº”å·²ä¿å­˜åˆ°: {raw_output_file}")
    
    # æå–derivationå’Œvalue
    derivation = "æœªæ‰¾åˆ°æ¨å¯¼è¿‡ç¨‹"
    value = None
    
    # è§£æå“åº”ä¸­çš„JSONå¯¹è±¡
    for line in response.splitlines():
        line = line.strip()
        if line.startswith("{") and line.endswith("}"):
            try:
                obj = json.loads(line)
                if "derivation" in obj:
                    derivation = obj["derivation"]
                    print("\næ¨å¯¼è¿‡ç¨‹:\n" + "-"*50)
                    print(derivation)
                    print("-"*50)
                if "value" in obj:
                    value = float(obj["value"])
                    print(f"\né¢„æµ‹å€¼: {value}\n")
            except (json.JSONDecodeError, TypeError, ValueError):
                print(f"[WARN] æ— æ³•è§£ææˆ–è½¬æ¢è¡Œï¼Œæˆ–å€¼æ— æ•ˆ: {line}")
                continue
    
    # å¦‚æœLLMæœªèƒ½æä¾›æœ‰æ•ˆæ•°å€¼ï¼Œåˆ™æ— æ³•ç»§ç»­è¯„ä¼°
    if value is None:
        print(f"[WARN] æœªèƒ½ä»LLMå“åº”ä¸­æå–æœ‰æ•ˆçš„é¢„æµ‹æ•°å€¼ã€‚è·³è¿‡å¯¹å®éªŒ '{exp_id}' çš„è¯„ä¼°ã€‚")
        return None

    # è®¡ç®—ä¸å®éªŒå€¼çš„åå·®ï¼ˆä½¿ç”¨åˆå¹¶åçš„å®Œæ•´å®éªŒæ•°æ®ï¼‰
    chi2 = None
    chi2_corrected = None
    success = None  # æ–°å¢ï¼šé¢„æµ‹æˆåŠŸæ ‡å¿—
    success_corrected = None  # ä»ªå™¨ä¿®æ­£åçš„æˆåŠŸæ ‡å¿—
    chi2_threshold = args.chi2_threshold if hasattr(args, 'chi2_threshold') else 4.0  # é»˜è®¤Ï‡Â²é˜ˆå€¼ä¸º4
    
    # åˆå§‹åŒ–ä»ªå™¨ä¿®æ­£å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    corrector = None
    correction_result = None
    if getattr(args, 'use_instrument_correction', False):
        corrector = InstrumentCorrector()
    
    if value is not None:
        measured = complete_exp.get("measured", {}).get("value")
        sigma = complete_exp.get("measured", {}).get("sigma")
        if measured is not None and sigma is not None:
            # åŸå§‹Ï‡Â²è®¡ç®—ï¼ˆæ— ä»ªå™¨ä¿®æ­£ï¼‰
            chi2 = ((value - measured) / sigma) ** 2
            success = chi2 < chi2_threshold
            
            # ä»ªå™¨ä¿®æ­£è¯„ä¼°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if corrector is not None and corrected_setup_exp is not None:
                # ä½¿ç”¨corrected_setup_expä¸­çš„ä»ªå™¨å‚æ•°è¿›è¡Œä¿®æ­£
                correction_result = corrector.evaluate_with_correction(
                    value, corrected_setup_exp, {"value": measured, "sigma": sigma}
                )
                chi2_corrected = correction_result["chi2_corrected"]
                success_corrected = chi2_corrected < chi2_threshold
                
                print(f"åŸå§‹Ï‡Â²å€¼: {chi2:.4f}")
                print(f"ä¿®æ­£åÏ‡Â²å€¼: {chi2_corrected:.4f}")
                print(f"ç†è®ºé¢„æµ‹å€¼: {value:.4f}")
                print(f"ä»ªå™¨ä¿®æ­£åé¢„æµ‹å€¼: {correction_result['corrected_prediction']:.4f}")
                print(f"å®éªŒæµ‹é‡å€¼: {measured:.4f}")
                print(f"åŸå§‹åå·®: {abs(value - measured):.4f}")
                print(f"ä¿®æ­£ååå·®: {abs(correction_result['corrected_prediction'] - measured):.4f}")
                print(f"åŸå§‹é¢„æµ‹ç»“æœ: {'æˆåŠŸ' if success else 'å¤±è´¥'} (Ï‡Â²é˜ˆå€¼={chi2_threshold})")
                print(f"ä¿®æ­£åé¢„æµ‹ç»“æœ: {'æˆåŠŸ' if success_corrected else 'å¤±è´¥'} (Ï‡Â²é˜ˆå€¼={chi2_threshold})")
                
                # æ˜¾ç¤ºä»ªå™¨å‚æ•°
                if "instrument_corrections" in corrected_setup_exp:
                    inst_params = correction_result["instrument_params"]
                    print(f"ä»ªå™¨å‚æ•°: Î·={inst_params['detection_efficiency']:.3f}, B={inst_params['background_noise']:.3f}, sys={inst_params['systematic_bias']:.3f}")
            else:
                # ä¼ ç»Ÿæ¨¡å¼è¾“å‡º
                print(f"Ï‡Â²å€¼: {chi2:.4f}")
                print(f"ä¸å®éªŒå€¼ {measured} çš„åå·®: {abs(value - measured):.4f}")
                print(f"é¢„æµ‹ç»“æœ: {'æˆåŠŸ' if success else 'å¤±è´¥'} (Ï‡Â²é˜ˆå€¼={chi2_threshold})")
    
    # åˆ›å»ºåŒ…å«æ¨å¯¼å’Œç»“æœçš„ç»“æ„åŒ–è¾“å‡º
    structured_output = {
        "theory_name": theory_name,
        "experiment_id": exp_id,
        "derivation": derivation,
        "predicted_value": float(value),  # ç¡®ä¿æ˜¯PythonåŸç”Ÿfloat
        "measured_value": float(measured_data[exp_id]["value"]),
        "sigma": float(measured_data[exp_id]["sigma"]),
        "chi2": float(chi2),
        "success": bool(success),  # ç¡®ä¿æ˜¯PythonåŸç”Ÿbool
        "chi2_threshold": float(chi2_threshold),
        "model_info": {
            "source": args.model_source,
            "name": args.model_name,
            "temperature": float(args.temperature)
        }
    }
    
    # å¦‚æœä½¿ç”¨äº†ä»ªå™¨ä¿®æ­£ï¼Œæ·»åŠ ç›¸å…³ä¿¡æ¯
    if correction_result is not None:
        structured_output.update({
            "corrected_predicted_value": float(correction_result["corrected_prediction"]),
            "total_sigma": float(correction_result["total_sigma"]),
            "chi2_corrected": float(chi2_corrected),
            "success_corrected": bool(success_corrected),  # ç¡®ä¿æ˜¯PythonåŸç”Ÿbool
            "instrument_correction": correction_result["instrument_params"]
        })
    
    # ä¿å­˜ç»“æ„åŒ–è¾“å‡º
    output_dir = os.path.dirname(output_file)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
        print(f"[INFO] å·²åˆ›å»ºç›®å½•: {output_dir}")

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(structured_output, f, ensure_ascii=False, indent=2)
    print(f"\n[INFO] è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    return structured_output

def load_experiments_from_directory(experiment_dir, use_instrument_correction=False):
    """ä»ç›®å½•ä¸­åŠ è½½æ‰€æœ‰å®éªŒæ–‡ä»¶ï¼ŒåŒ¹é…è®¾ç½®å’Œæµ‹é‡æ•°æ®"""
    print(f"[INFO] æ­£åœ¨ä»ç›®å½• {experiment_dir} åŠ è½½å®éªŒ...")
    
    # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
    experiment_files = glob.glob(os.path.join(experiment_dir, "*.json"))
    
    # åˆ†ç¦»è®¾ç½®å’Œæµ‹é‡æ–‡ä»¶
    setup_files = {}
    setup_corrected_files = {}
    measured_files = {}
    
    # æ–°å¢: ç›´æ¥åŠ è½½æ‰€æœ‰æµ‹é‡æ•°æ®åˆ°ä¸€ä¸ªå­—å…¸
    measured_db = {}

    for file_path in experiment_files:
        filename = os.path.basename(file_path)
        
        # ç»Ÿä¸€åŠ è½½æ‰€æœ‰æµ‹é‡æ•°æ®
        if "_measured.json" in filename:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                exp_id = data.get("id")
                if exp_id:
                    measured_db[exp_id] = data
            except Exception as e:
                print(f"[WARN] æ— æ³•åŠ è½½æµ‹é‡æ–‡ä»¶ {filename}: {e}")
            continue

        # åˆ†ç¦»ä¸åŒç±»å‹çš„è®¾ç½®æ–‡ä»¶
        if "setup_corrected" in filename:
            match = re.match(r"(.+?)_setup_corrected\.json", filename)
            if match:
                setup_corrected_files[match.group(1)] = file_path
        elif "setup" in filename and filename.endswith("_setup.json"):
            match = re.match(r"(.+?)_setup\.json", filename)
            if match:
                setup_files[match.group(1)] = file_path

    # ç°åœ¨ï¼Œæ ¹æ®æ¨¡å¼ç¡®å®šè¦åŠ è½½å“ªäº›å®éªŒ
    experiments_to_load = {}
    instrument_setups = {}

    if use_instrument_correction:
        # ä»ªå™¨ä¿®æ­£æ¨¡å¼: éœ€è¦ setup, setup_corrected, å’Œ measured æ•°æ®
        valid_ids = set(setup_files.keys()) & set(setup_corrected_files.keys()) & set(measured_db.keys())
        print(f"[INFO] ä»ªå™¨ä¿®æ­£æ¨¡å¼ï¼šæ‰¾åˆ° {len(valid_ids)} ä¸ªå®Œæ•´å®éªŒæ•°æ®é›†")
        for exp_id in valid_ids:
            try:
                with open(setup_files[exp_id], 'r') as f:
                    experiments_to_load[exp_id] = json.load(f)
                with open(setup_corrected_files[exp_id], 'r') as f:
                    instrument_setups[exp_id] = json.load(f)
                print(f"[INFO] å·²åŠ è½½å®éªŒ: {exp_id} (åŸå§‹+ä¿®æ­£è®¾ç½®)")
            except Exception as e:
                print(f"[ERROR] åŠ è½½ä»ªå™¨ä¿®æ­£å®éªŒ {exp_id} æ—¶å‡ºé”™: {e}")

    else:
        # æ ‡å‡†æ¨¡å¼: éœ€è¦ setup (æˆ– setup_corrected) å’Œ measured æ•°æ®
        available_setup_ids = set(setup_files.keys()) | set(setup_corrected_files.keys())
        valid_ids = available_setup_ids & set(measured_db.keys())
        print(f"[INFO] æ ‡å‡†æ¨¡å¼ï¼šæ‰¾åˆ° {len(valid_ids)} ä¸ªå®Œæ•´å®éªŒæ•°æ®é›†")
        for exp_id in valid_ids:
            try:
                # ä¼˜å…ˆä½¿ç”¨ corrected æ–‡ä»¶
                file_to_load = setup_corrected_files.get(exp_id, setup_files.get(exp_id))
                file_type = "corrected" if exp_id in setup_corrected_files else "original"
                with open(file_to_load, 'r') as f:
                    experiments_to_load[exp_id] = json.load(f)
                print(f"[INFO] å·²åŠ è½½å®éªŒ: {exp_id} (ä½¿ç”¨{file_type}è®¾ç½®)")
            except Exception as e:
                print(f"[ERROR] åŠ è½½æ ‡å‡†å®éªŒ {exp_id} æ—¶å‡ºé”™: {e}")
                
    return experiments_to_load, instrument_setups, measured_db

def get_file_list(path_spec):
    """æ ¹æ®è·¯å¾„è§„æ ¼ï¼ˆå•ä¸ªæ–‡ä»¶ã€ç›®å½•ã€é€šé…ç¬¦ï¼‰è·å–æ–‡ä»¶åˆ—è¡¨"""
    if '*' in path_spec or '?' in path_spec:
        return glob.glob(path_spec)
    elif os.path.isdir(path_spec):
        return [os.path.join(path_spec, f) for f in os.listdir(path_spec)]
    elif os.path.isfile(path_spec):
        return [path_spec]
    return []

async def main():
    # å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(
        description="Quantum Theory Evaluator - A tool to evaluate quantum theories against experiments using LLMs.",
        formatter_class=argparse.RawTextHelpFormatter
    )
    
    # --- ä¸»è¦è¿è¡Œå‚æ•° ---
    main_group = parser.add_argument_group('Main Parameters')
    main_group.add_argument("--theory_path", type=str, required=True, help="Path to the JSON file or directory containing theory definitions.")
    main_group.add_argument("--experiment_dir", type=str, required=True, help="Directory containing experiment setup and data files.")
    main_group.add_argument("--output_dir", type=str, default="evaluation_results", help="Directory to save evaluation results.")
    main_group.add_argument("--schema_version", type=str, default="any", help="The schema version of the theories to be loaded ('any' to load all).")
    main_group.add_argument("--max_theories", type=int, default=None, help="Maximum number of theories to evaluate.")
    main_group.add_argument("--max_experiments", type=int, default=None, help="Maximum number of experiments to run for each theory.")
    main_group.add_argument("--chi2_threshold", type=float, default=4.0, help="Chi-squared threshold for determining prediction success.")
    main_group.add_argument("--use_instrument_correction", action='store_true', help="Enable instrument correction model.")
    main_group.add_argument("--start-at-index", type=int, default=0, help="0-based index of the theory to start evaluation from.")

    # --- LLM ç›¸å…³å‚æ•° ---
    model_group = parser.add_argument_group('LLM Configuration')
    model_group.add_argument("--model_source", type=str, default="deepseek", choices=["openai", "deepseek", "google"], help="LLM provider.")
    model_group.add_argument("--model_name", type=str, default="deepseek-reasoner", help="Specific model name.")
    model_group.add_argument("--temperature", type=float, default=0.1, help="Sampling temperature for the LLM (0.0 to 1.0).")

    # --- è§’è‰²è¯„ä¼°å‚æ•° ---
    role_eval_group = parser.add_argument_group('Role-playing Evaluation (Optional)')
    role_eval_group.add_argument("--run_role_evaluation", action='store_true', help="Run role-playing evaluation after experimental evaluation.")
    role_eval_group.add_argument("--role_success_threshold", type=float, default=0.75, help="Success rate threshold for a theory to be passed to role evaluation.")
    role_eval_group.add_argument("--role_model_source", type=str, default="openai", choices=["openai", "deepseek", "google"], help="LLM provider for role evaluation.")
    role_eval_group.add_argument("--role_model_name", type=str, default="gpt-4o-mini", help="Specific model name for role evaluation.")

    args = parser.parse_args()

    # --- 1. è®¾ç½® ---
    # åˆ›å»ºå”¯ä¸€çš„è¾“å‡ºç›®å½•
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    run_output_dir = os.path.join(args.output_dir, f"run_{timestamp}_{args.model_name.replace('/', '_')}")
    os.makedirs(run_output_dir, exist_ok=True)
    print(f"[SETUP] Results will be saved in: {run_output_dir}")

    # åˆå§‹åŒ–LLM
    llm = LLMInterface(model_source=args.model_source, model_name=args.model_name)
    print(f"[SETUP] Initialized LLM: {args.model_source}/{args.model_name}")

    # --- 2. åŠ è½½æ•°æ® ---
    print(f"\n[LOAD] Loading theories from: {args.theory_path}")
    schema_to_load = None if args.schema_version.lower() == 'any' else args.schema_version
    theories = load_theories_from_sources(args.theory_path, schema_version=schema_to_load)
    if not theories:
        print("[ERROR] No theories loaded. Exiting.")
        return
    
    # é™åˆ¶ç†è®ºæ•°é‡
    if args.max_theories is not None and args.max_theories > 0:
        theories = dict(list(theories.items())[:args.max_theories])
    print(f"[LOAD] Loaded {len(theories)} theories for evaluation: {list(theories.keys())}")

    # åŠ è½½å®éªŒ
    print(f"\n[LOAD] Loading experiments from: {args.experiment_dir}")
    
    experiments, instrument_setups, measured_data = load_experiments_from_directory(
        args.experiment_dir, args.use_instrument_correction
    )
    if not experiments:
        print("[ERROR] No experiments loaded. Exiting.")
        return
        
    # é™åˆ¶å®éªŒæ•°é‡
    if args.max_experiments is not None and args.max_experiments > 0:
        experiments = dict(list(experiments.items())[:args.max_experiments])
    print(f"[LOAD] Loaded {len(experiments)} experiments for evaluation: {list(experiments.keys())}")


    # --- 3. è¿è¡Œè¯„ä¼° ---
    print(f"\n[EVAL] Starting evaluation...")
    all_results = []
    
    for i, (theory_name, theory) in enumerate(theories.items()):
        if i < args.start_at_index:
            continue
            
        print(f"\n--- Evaluating Theory {i+1}/{len(theories)}: {theory_name} ---")
        theory_results = []
        
        # ä¸ºæ¯ä¸ªç†è®ºåˆ›å»ºä¸€ä¸ªå­ç›®å½•
        theory_output_dir = os.path.join(run_output_dir, theory_name.replace(" ", "_").lower())
        os.makedirs(theory_output_dir, exist_ok=True)

        for j, (exp_id, setup_exp) in enumerate(experiments.items()):
            print(f"--- Running Experiment {j+1}/{len(experiments)}: {exp_id} ---")

            # ç¡®å®šä»ªå™¨ä¿®æ­£çš„è®¾ç½®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            corrected_setup = instrument_setups.get(exp_id) if args.use_instrument_correction else None
            
            result = await evaluate_theory_experiment(
                theory=theory,
                setup_exp=setup_exp,
                measured_data=measured_data,
                llm=llm,
                args=args,
                # å°†æ‰€æœ‰å­è¾“å‡ºå®šå‘åˆ°ç†è®ºçš„ç›®å½•
                output_prefix=f"{theory_name.replace(' ', '_').lower()}_vs_{exp_id.replace(' ', '_').lower()}",
                corrected_setup_exp=corrected_setup
            )
            
            if result:
                theory_results.append(result)
        
        # ä¿å­˜è¯¥ç†è®ºçš„æ‰€æœ‰è¯„ä¼°ç»“æœ
        if theory_results:
            theory_summary_file = os.path.join(theory_output_dir, "_summary.json")
            with open(theory_summary_file, "w", encoding="utf-8") as f:
                json.dump(theory_results, f, ensure_ascii=False, indent=2)
            print(f"[SUMMARY] Theory '{theory_name}' summary saved to: {theory_summary_file}")
            all_results.extend(theory_results)

    # --- 4. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š ---
    if all_results:
        # å¯¹æœ€ç»ˆç»“æœè¿›è¡Œæ’å
        # ä½¿ç”¨ä¿®æ­£åçš„chi2ï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨åŸå§‹chi2
        use_corrected = args.use_instrument_correction and all(
            'chi2_corrected' in r for r in all_results
        )
        
        # æ±‡æ€»æ¯ä¸ªç†è®ºçš„è¡¨ç°
        theory_performance = {}
        for result in all_results:
            t_name = result['theory_name']
            if t_name not in theory_performance:
                theory_performance[t_name] = {
                    'success_count': 0,
                    'total_count': 0,
                    'chi2_sum': 0,
                    'chi2_list': []
                }
            
            theory_performance[t_name]['total_count'] += 1
            
            # åˆ¤æ–­æˆåŠŸä¸å¦
            is_success = result.get('success_corrected', result['success']) if use_corrected else result['success']
            if is_success:
                theory_performance[t_name]['success_count'] += 1

            # ç´¯åŠ chi2
            chi2_to_add = result.get('chi2_corrected', result['chi2']) if use_corrected else result['chi2']
            if chi2_to_add is not None:
                theory_performance[t_name]['chi2_sum'] += chi2_to_add
                theory_performance[t_name]['chi2_list'].append(chi2_to_add)

        # è®¡ç®—æˆåŠŸç‡å’Œå¹³å‡chi2
        ranked_theories = []
        for t_name, perf_data in theory_performance.items():
            success_rate = perf_data['success_count'] / perf_data['total_count'] if perf_data['total_count'] > 0 else 0
            average_chi2 = perf_data['chi2_sum'] / len(perf_data['chi2_list']) if perf_data['chi2_list'] else float('inf')
            
            ranked_theories.append({
                "theory_name": t_name,
                "success_rate": success_rate,
                "average_chi2": average_chi2,
                "experiments_count": perf_data['total_count']
            })
            
        # æŒ‰æˆåŠŸç‡é™åºï¼Œå¹³å‡chi2å‡åºæ’åº
        ranked_theories.sort(key=lambda x: (-x['success_rate'], x['average_chi2']))

        final_summary_file = os.path.join(run_output_dir, "final_evaluation_summary.json")
        with open(final_summary_file, "w", encoding="utf-8") as f:
            json.dump(ranked_theories, f, ensure_ascii=False, indent=2)
        print(f"\n[FINAL] Overall experimental evaluation summary saved to: {final_summary_file}")

        # æ‰“å°å®éªŒè¯„ä¼°æ’å
        print("\n" + "="*100)
        print("ğŸ“Š å®éªŒè¯„ä¼°æ’å")
        print("="*100)
        print(f"{'æ’å':<5} {'ç†è®ºåç§°':<40} {'æˆåŠŸç‡':<15} {'å¹³å‡Ï‡Â²':<15} {'å®éªŒæ•°':<10}")
        print("-"*100)
        for i, rank in enumerate(ranked_theories, 1):
            print(f"{i:<5} {rank['theory_name']:<40} {rank['success_rate']*100:14.1f}% {rank['average_chi2']:<15.4f} {rank['experiments_count']:<10}")
        print("-"*100)
        
        # --- 5. è‡ªåŠ¨è¿è¡Œè§’è‰²è¯„ä¼° (å¦‚æœå¯ç”¨) ---
        if args.run_role_evaluation:
            # ç­›é€‰é«˜æˆåŠŸç‡ç†è®º
            high_success_theories = [
                r for r in ranked_theories 
                if r['success_rate'] >= args.role_success_threshold
            ]
            
            if high_success_theories:
                # è°ƒç”¨è§’è‰²è¯„ä¼°æ¨¡å—
                await run_role_evaluation_for_theories(
                    high_success_theories=high_success_theories,
                    all_theories_definitions=theories, # ä½¿ç”¨å·²åŠ è½½çš„ç†è®ºå®šä¹‰
                    output_dir=run_output_dir, # åœ¨åŒä¸€è¿è¡Œç›®å½•ä¸‹è¾“å‡º
                    model_source=args.role_model_source,
                    model_name=args.role_model_name
                )
            else:
                print(f"\n[INFO] æ²¡æœ‰ç†è®ºè¾¾åˆ° {args.role_success_threshold*100:.0f}% çš„æˆåŠŸç‡é˜ˆå€¼ï¼Œè·³è¿‡è§’è‰²è¯„ä¼°ã€‚")

    else:
        print("\n[FINAL] No evaluations were successfully completed.")

    print("\nEvaluation run finished.")


if __name__ == "__main__":
    # Windowså¹³å°å…¼å®¹æ€§è®¾ç½®
    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n[INFO] è¿è¡Œè¢«ç”¨æˆ·ä¸­æ–­ã€‚")
    except Exception as e:
        print(f"[CRITICAL ERROR] å‘ç”Ÿæœªå¤„ç†çš„å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()