#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åŸºäºå·²æœ‰å®éªŒè¯„ä¼°ç»“æœçš„è‡ªåŠ¨è§’è‰²è¯„ä¼°è„šæœ¬

è¯»å–ä¹‹å‰çš„å®éªŒè¯„ä¼°æ’åç»“æœï¼Œè‡ªåŠ¨å¯¹æˆåŠŸç‡â‰¥80%çš„ç†è®ºè¿›è¡Œå¤šè§’è‰²è¯„ä¼°
"""

import os
import sys
import json
import argparse
import asyncio

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from theory_validation.agent_validation.theory_evaluator import TheoryEvaluator
from theory_generation.llm_interface import LLMInterface

async def auto_role_evaluation(ranking_file, theories_file, output_dir, 
                              success_threshold=0.8, model_source="deepseek", 
                              model_name="deepseek-chat"):
    """
    åŸºäºå®éªŒè¯„ä¼°æ’åç»“æœè¿›è¡Œè‡ªåŠ¨è§’è‰²è¯„ä¼°
    
    Args:
        ranking_file: ç†è®ºæ’åæ–‡ä»¶è·¯å¾„ (theory_rankings.json)
        theories_file: ç†è®ºå®šä¹‰æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        success_threshold: æˆåŠŸç‡é˜ˆå€¼ (é»˜è®¤0.8å³80%)
        model_source: LLMæ¨¡å‹æ¥æº
        model_name: LLMæ¨¡å‹åç§°
    """
    print(f"[INFO] ä½¿ç”¨{model_source}æ¨¡å‹: {model_name}")
    print(f"[INFO] æˆåŠŸç‡é˜ˆå€¼: {success_threshold*100:.0f}%")
    
    # 1. è¯»å–ç†è®ºæ’åç»“æœ
    try:
        with open(ranking_file, 'r', encoding='utf-8') as f:
            theory_rankings = json.load(f)
        print(f"[INFO] å·²åŠ è½½ {len(theory_rankings)} ä¸ªç†è®ºçš„æ’åç»“æœ")
    except Exception as e:
        print(f"[ERROR] è¯»å–æ’åæ–‡ä»¶å¤±è´¥: {str(e)}")
        return
    
    # 2. ç­›é€‰é«˜æˆåŠŸç‡ç†è®º
    high_success_theories = [
        rank for rank in theory_rankings 
        if rank['success_rate'] >= success_threshold
    ]
    
    if not high_success_theories:
        print(f"[INFO] æ²¡æœ‰ç†è®ºè¾¾åˆ°{success_threshold*100:.0f}%æˆåŠŸç‡é˜ˆå€¼")
        return
    
    print(f"[INFO] å‘ç° {len(high_success_theories)} ä¸ªæˆåŠŸç‡â‰¥{success_threshold*100:.0f}%çš„ç†è®º:")
    for theory in high_success_theories:
        print(f"  - {theory['theory_name']}: {theory['success_rate']*100:.1f}%")
    
    # 3. è¯»å–ç†è®ºå®šä¹‰
    try:
        with open(theories_file, 'r', encoding='utf-8') as f:
            theories_data = json.load(f)
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        if isinstance(theories_data, list):
            theories = {theory['name']: theory for theory in theories_data}
        else:
            theories = theories_data
            
        print(f"[INFO] å·²åŠ è½½ {len(theories)} ä¸ªç†è®ºå®šä¹‰")
    except Exception as e:
        print(f"[ERROR] è¯»å–ç†è®ºæ–‡ä»¶å¤±è´¥: {str(e)}")
        return
    
    # 4. åˆå§‹åŒ–LLMå’Œè§’è‰²è¯„ä¼°å™¨
    try:
        llm = LLMInterface(model_name=model_name, model_source=model_source)
        role_evaluator = TheoryEvaluator(llm)
        print(f"[INFO] å·²åˆå§‹åŒ–è§’è‰²è¯„ä¼°å™¨")
    except Exception as e:
        print(f"[ERROR] åˆå§‹åŒ–è¯„ä¼°å™¨å¤±è´¥: {str(e)}")
        return
    
    # 5. åˆ›å»ºè¾“å‡ºç›®å½•
    role_output_dir = os.path.join(output_dir, "role_evaluations")
    os.makedirs(role_output_dir, exist_ok=True)
    
    # 6. æ‰§è¡Œè§’è‰²è¯„ä¼°
    role_results = []
    
    for rank in high_success_theories:
        theory_name = rank['theory_name']
        
        if theory_name not in theories:
            print(f"[WARNING] æœªæ‰¾åˆ°ç†è®º '{theory_name}' çš„å®šä¹‰ï¼Œè·³è¿‡")
            continue
            
        theory_data = theories[theory_name]
        
        print(f"\n[INFO] æ­£åœ¨å¯¹ç†è®º '{theory_name}' è¿›è¡Œè§’è‰²è¯„ä¼°...")
        print(f"       å®éªŒæˆåŠŸç‡: {rank['success_rate']*100:.1f}%, å¹³å‡Ï‡Â²: {rank['average_chi2']:.4f}")
        
        try:
            # æ‰§è¡Œè§’è‰²è¯„ä¼°
            # å°†ç†è®ºæ•°æ®é€‚é…ä¸ºè§’è‰²è¯„ä¼°å™¨æœŸæœ›çš„æ ¼å¼
            adapted_theory = {
                'name': theory_data.get('name', theory_name),
                'core_principles': theory_data.get('summary', ''),
                'detailed_description': f"å“²å­¦ç«‹åœº: {theory_data.get('philosophy', {}).get('ontology', '')}\\næµ‹é‡è§£é‡Š: {theory_data.get('philosophy', {}).get('measurement', '')}",
                'quantum_phenomena_explanation': {
                    'wave_function_collapse': theory_data.get('philosophy', {}).get('measurement', ''),
                    'measurement_problem': theory_data.get('philosophy', {}).get('measurement', ''),
                    'non_locality': theory_data.get('philosophy', {}).get('ontology', '')
                },
                'philosophical_stance': theory_data.get('philosophy', {}),
                'mathematical_formulation': theory_data.get('formalism', {}),
                'parameters': theory_data.get('parameters', {}),
                'semantics': theory_data.get('semantics', {})
            }
            
            # åˆå§‹åŒ–ç»“æœ
            role_result = {
                'theory_name': theory_name,
                'theory_id': f"AUTO_{hash(theory_name)}",
                'evaluations': {},
                'avg_chi2': 0,
                'conflicts': [],
                'detailed_results': []
            }
            
            # ç›´æ¥è°ƒç”¨å„ä¸ªè§’è‰²çš„è¯„ä¼°æ–¹æ³•
            for role_id, role_info in role_evaluator.evaluation_roles.items():
                print(f"       æ­£åœ¨è¿›è¡Œ{role_info['name']}è¯„ä¼°...")
                eval_result = await role_evaluator._evaluate_as_role(adapted_theory, role_id, role_info)
                role_result['evaluations'][role_id] = eval_result
                score = eval_result.get('score', 0)
                print(f"       {role_info['name']}è¯„ä¼°å®Œæˆï¼Œå¾—åˆ†: {score}/10")
            
            # å¦‚æœæœ‰è¯„ä¼°ç»“æœï¼Œç”Ÿæˆæ€»ç»“
            if role_result['evaluations']:
                summary = await role_evaluator._generate_evaluation_summary(adapted_theory, role_result['evaluations'])
                role_result['summary'] = summary
            
            # æ·»åŠ å®éªŒæˆåŠŸç‡ä¿¡æ¯
            role_result['experiment_success_rate'] = rank['success_rate']
            role_result['average_chi2'] = rank['average_chi2']
            role_result['experiments_count'] = rank['experiments_count']
            
            role_results.append(role_result)
            
            # ä¿å­˜å•ä¸ªç†è®ºçš„è§’è‰²è¯„ä¼°ç»“æœ
            theory_role_file = os.path.join(
                role_output_dir,
                f"{theory_name.replace(' ', '_').lower()}_role_evaluation.json"
            )
            with open(theory_role_file, "w", encoding="utf-8") as f:
                json.dump(role_result, f, ensure_ascii=False, indent=2)
            
            print(f"[INFO] ç†è®º '{theory_name}' çš„è§’è‰²è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {theory_role_file}")
            
        except Exception as e:
            print(f"[ERROR] è¯„ä¼°ç†è®º '{theory_name}' çš„è§’è‰²æ—¶å‡ºé”™: {str(e)}")
    
    # 7. ä¿å­˜ç»¼åˆç»“æœå’Œæ’å
    if role_results:
        # ä¿å­˜è§’è‰²è¯„ä¼°æ±‡æ€»
        role_summary_file = os.path.join(role_output_dir, "role_evaluation_summary.json")
        with open(role_summary_file, "w", encoding="utf-8") as f:
            json.dump(role_results, f, ensure_ascii=False, indent=2)
        
        # è®¡ç®—ç»¼åˆæ’å
        combined_rankings = []
        for role_result in role_results:
            theory_name = role_result.get('theory_name', 'Unknown Theory')
            
            # è®¡ç®—è§’è‰²è¯„ä¼°å¹³å‡åˆ†
            role_scores = []
            if 'evaluations' in role_result and isinstance(role_result['evaluations'], dict):
                for role, eval_data in role_result['evaluations'].items():
                    if isinstance(eval_data, dict) and 'score' in eval_data:
                        score = eval_data['score']
                        if score is not None:
                            role_scores.append(score)
            
            avg_role_score = sum(role_scores) / len(role_scores) if role_scores else 0
            
            # ç»¼åˆè¯„åˆ† = å®éªŒæˆåŠŸç‡ * 0.6 + è§’è‰²è¯„ä¼°åˆ† * 0.4
            combined_score = (
                role_result['experiment_success_rate'] * 0.6 + 
                avg_role_score / 10.0 * 0.4  # è§’è‰²è¯„ä¼°åˆ†é€šå¸¸æ˜¯1-10åˆ†ï¼Œå½’ä¸€åŒ–åˆ°0-1
            )
            
            # æ”¶é›†è§’è‰²è¯¦ç»†è¯„åˆ†
            role_details = {}
            if 'evaluations' in role_result and isinstance(role_result['evaluations'], dict):
                for role, eval_data in role_result['evaluations'].items():
                    if isinstance(eval_data, dict) and 'score' in eval_data:
                        role_details[role] = eval_data.get('score', 0)
            
            combined_rankings.append({
                'theory_name': theory_name,
                'experiment_success_rate': role_result['experiment_success_rate'],
                'average_chi2': role_result['average_chi2'],
                'experiments_count': role_result['experiments_count'],
                'average_role_score': avg_role_score,
                'combined_score': combined_score,
                'role_details': role_details,
                'role_evaluation_status': role_result.get('status', 'unknown')
            })
        
        # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
        combined_rankings.sort(key=lambda x: -x['combined_score'])
        
        # ä¿å­˜ç»¼åˆæ’å
        combined_ranking_file = os.path.join(role_output_dir, "combined_rankings.json")
        with open(combined_ranking_file, "w", encoding="utf-8") as f:
            json.dump(combined_rankings, f, ensure_ascii=False, indent=2)
        
        print(f"\n[INFO] è§’è‰²è¯„ä¼°æ±‡æ€»å·²ä¿å­˜åˆ°: {role_summary_file}")
        print(f"[INFO] ç»¼åˆæ’åå·²ä¿å­˜åˆ°: {combined_ranking_file}")
        
        # æ‰“å°ç»¼åˆæ’å
        print("\n" + "="*120)
        print("ğŸ† ç»¼åˆæ’åï¼ˆå®éªŒæˆåŠŸç‡ 60% + è§’è‰²è¯„ä¼° 40%ï¼‰")
        print("="*120)
        print(f"{'æ’å':<4} {'ç†è®ºåç§°':<30} {'å®éªŒæˆåŠŸç‡':<12} {'è§’è‰²å¹³å‡åˆ†':<12} {'ç»¼åˆè¯„åˆ†':<12} {'è¯¦ç»†è§’è‰²è¯„åˆ†'}")
        print("-"*120)
        
        for i, rank in enumerate(combined_rankings, 1):
            role_detail = ", ".join([f"{role}:{score:.1f}" for role, score in rank['role_details'].items() if score > 0])
            print(f"{i:<4} {rank['theory_name']:<30} {rank['experiment_success_rate']*100:>8.1f}% "
                  f"{rank['average_role_score']:>10.1f} {rank['combined_score']:>10.3f} {role_detail}")
        
        print("-"*120)
        print(f"å…±è¯„ä¼°äº† {len(combined_rankings)} ä¸ªé«˜æˆåŠŸç‡ç†è®º")
        
        # è¾“å‡ºæœ€ä½³ç†è®ºçš„è¯¦ç»†ä¿¡æ¯
        if combined_rankings:
            best_theory = combined_rankings[0]
            print(f"\nğŸ¥‡ æœ€ä½³ç†è®º: {best_theory['theory_name']}")
            print(f"   å®éªŒæˆåŠŸç‡: {best_theory['experiment_success_rate']*100:.1f}%")
            print(f"   å¹³å‡Ï‡Â²å€¼: {best_theory['average_chi2']:.4f}")
            print(f"   è§’è‰²è¯„ä¼°å¹³å‡åˆ†: {best_theory['average_role_score']:.1f}/10")
            print(f"   ç»¼åˆè¯„åˆ†: {best_theory['combined_score']:.3f}")
        
    else:
        print("[WARNING] æ²¡æœ‰æˆåŠŸå®Œæˆçš„è§’è‰²è¯„ä¼°ç»“æœ")

def main():
    parser = argparse.ArgumentParser(description="åŸºäºå®éªŒè¯„ä¼°ç»“æœçš„è‡ªåŠ¨è§’è‰²è¯„ä¼°")
    parser.add_argument("--ranking_file", required=True, 
                       help="ç†è®ºæ’åæ–‡ä»¶è·¯å¾„ (theory_rankings.json)")
    parser.add_argument("--theories_file", required=True,
                       help="ç†è®ºå®šä¹‰æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", required=True,
                       help="è¾“å‡ºç›®å½•è·¯å¾„")
    parser.add_argument("--success_threshold", type=float, default=0.8,
                       help="æˆåŠŸç‡é˜ˆå€¼ (é»˜è®¤0.8å³80%)")
    parser.add_argument("--model_source", default="deepseek",
                       help="LLMæ¨¡å‹æ¥æº (ä¾‹å¦‚: openai, deepseek)")
    parser.add_argument("--model_name", default="deepseek-chat",
                       help="LLMæ¨¡å‹åç§° (ä¾‹å¦‚: gpt-4, deepseek-chat)")
    
    args = parser.parse_args()
    
    # éªŒè¯è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.ranking_file):
        print(f"[ERROR] æ’åæ–‡ä»¶ä¸å­˜åœ¨: {args.ranking_file}")
        return
    
    if not os.path.exists(args.theories_file):
        print(f"[ERROR] ç†è®ºæ–‡ä»¶ä¸å­˜åœ¨: {args.theories_file}")
        return
    
    asyncio.run(auto_role_evaluation(
        args.ranking_file,
        args.theories_file,
        args.output_dir,
        args.success_threshold,
        args.model_source,
        args.model_name
    ))

if __name__ == "__main__":
    main() 