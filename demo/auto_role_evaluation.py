#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åŸºäºå·²æœ‰å®éªŒè¯„ä¼°ç»“æœçš„è‡ªåŠ¨è§’è‰²è¯„ä¼°è„šæœ¬

è¯»å–ä¹‹å‰çš„å®éªŒè¯„ä¼°æ’åç»“æœï¼Œè‡ªåŠ¨å¯¹æˆåŠŸç‡â‰¥80%çš„ç†è®ºè¿›è¡Œå¤šè§’è‰²è¯„ä¼°
"""

import os
import sys
import json
import argparse
import asyncio
from typing import List, Dict, Any

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from theory_validation.agent_validation.theory_evaluator import TheoryEvaluator
from theory_generation.llm_interface import LLMInterface

async def run_role_evaluation_for_theories(
    high_success_theories: List[Dict[str, Any]],
    all_theories_definitions: Dict[str, Any],
    output_dir: str,
    model_source: str = "deepseek",
    model_name: str = "deepseek-chat"
):
    """
    å¯¹ç»™å®šçš„é«˜æˆåŠŸç‡ç†è®ºåˆ—è¡¨è¿›è¡Œè§’è‰²è¯„ä¼°ã€‚

    Args:
        high_success_theories: ç»å®éªŒè¯„ä¼°ç­›é€‰å‡ºçš„é«˜æˆåŠŸç‡ç†è®ºä¿¡æ¯åˆ—è¡¨ã€‚
                                (e.g., [{'theory_name': 'T1', 'success_rate': 0.9, ...}, ...])
        all_theories_definitions: åŒ…å«æ‰€æœ‰ç†è®ºå®šä¹‰çš„å­—å…¸ã€‚
                                (e.g., {'T1': {'name': 'T1', 'philosophy': ...}, ...})
        output_dir: è§’è‰²è¯„ä¼°ç»“æœçš„è¾“å‡ºç›®å½•ã€‚
        model_source: LLMæ¨¡å‹æ¥æºã€‚
        model_name: LLMæ¨¡å‹åç§°ã€‚
    """
    print("\n" + "="*50)
    print("ğŸš€ å¼€å§‹è¿›è¡Œå¤šè§’è‰²è¯„ä¼°...")
    print("="*50)
    print(f"[INFO] æ”¶åˆ° {len(high_success_theories)} ä¸ªç†è®ºè¿›è¡Œè§’è‰²è¯„ä¼°ã€‚")
    print(f"[INFO] ä½¿ç”¨æ¨¡å‹: {model_source}/{model_name}")

    # 1. åˆå§‹åŒ–LLMå’Œè§’è‰²è¯„ä¼°å™¨
    try:
        llm = LLMInterface(model_name=model_name, model_source=model_source)
        role_evaluator = TheoryEvaluator(llm)
        print(f"[INFO] å·²åˆå§‹åŒ–è§’è‰²è¯„ä¼°å™¨")
    except Exception as e:
        print(f"[ERROR] åˆå§‹åŒ–è§’è‰²è¯„ä¼°å™¨å¤±è´¥: {str(e)}")
        return

    # 2. åˆ›å»ºè¾“å‡ºç›®å½•
    role_output_dir = os.path.join(output_dir, "role_evaluations")
    os.makedirs(role_output_dir, exist_ok=True)

    # 3. æ‰§è¡Œè§’è‰²è¯„ä¼°
    role_results = []

    for rank_info in high_success_theories:
        theory_name = rank_info['theory_name']

        if theory_name not in all_theories_definitions:
            print(f"[WARNING] æœªæ‰¾åˆ°ç†è®º '{theory_name}' çš„å®šä¹‰ï¼Œè·³è¿‡è§’è‰²è¯„ä¼°")
            continue

        theory_data = all_theories_definitions[theory_name]

        print(f"\n[INFO] æ­£åœ¨å¯¹ç†è®º '{theory_name}' è¿›è¡Œè§’è‰²è¯„ä¼°...")
        print(f"       å®éªŒæˆåŠŸç‡: {rank_info['success_rate']*100:.1f}%, å¹³å‡Ï‡Â²: {rank_info['average_chi2']:.4f}")

        try:
            # æ‰§è¡Œè§’è‰²è¯„ä¼°
            # å°†ç†è®ºæ•°æ®é€‚é…ä¸ºè§’è‰²è¯„ä¼°å™¨æœŸæœ›çš„æ ¼å¼
            adapted_theory = {
                'name': theory_data.get('name', theory_name),
                'core_principles': theory_data.get('summary', ''),
                'detailed_description': f"å“²å­¦ç«‹åœº: {theory_data.get('philosophy', {}).get('ontology', '')}\\næµ‹é‡è§£é‡Š: {theory_data.get('philosophy', {}).get('measurement', '')}",
                'quantum_phenomena_explanation': {
                    'wave_function_collapse': theory_data.get('philosophy', {}).get('measurement', ''),
                    'measurement_problem': theory_data.get('philosophy', {}).get('measurement', ''),
                    'non_locality': theory_data.get('philosophy', {}).get('ontology', '')
                },
                'philosophical_stance': theory_data.get('philosophy', {}),
                'mathematical_formulation': theory_data.get('formalism', {}),
                'parameters': theory_data.get('parameters', {}),
                'semantics': theory_data.get('semantics', {})
            }
            
            # åˆå§‹åŒ–ç»“æœ
            role_result = {
                'theory_name': theory_name,
                'theory_id': f"AUTO_{hash(theory_name)}",
                'evaluations': {},
                'avg_chi2': 0,
                'conflicts': [],
                'detailed_results': []
            }
            
            # ç›´æ¥è°ƒç”¨å„ä¸ªè§’è‰²çš„è¯„ä¼°æ–¹æ³•
            for role_id, role_info in role_evaluator.evaluation_roles.items():
                print(f"       æ­£åœ¨è¿›è¡Œ{role_info['name']}è¯„ä¼°...")
                eval_result = await role_evaluator._evaluate_as_role(adapted_theory, role_id, role_info)
                role_result['evaluations'][role_id] = eval_result
                score = eval_result.get('score', 0)
                print(f"       {role_info['name']}è¯„ä¼°å®Œæˆï¼Œå¾—åˆ†: {score}/10")
            
            # å¦‚æœæœ‰è¯„ä¼°ç»“æœï¼Œç”Ÿæˆæ€»ç»“
            if role_result['evaluations']:
                summary = await role_evaluator._generate_evaluation_summary(adapted_theory, role_result['evaluations'])
                role_result['summary'] = summary
            
            # æ·»åŠ å®éªŒæˆåŠŸç‡ä¿¡æ¯
            role_result['experiment_success_rate'] = rank_info['success_rate']
            role_result['average_chi2'] = rank_info['average_chi2']
            role_result['experiments_count'] = rank_info['experiments_count']
            
            role_results.append(role_result)
            
            # ä¿å­˜å•ä¸ªç†è®ºçš„è§’è‰²è¯„ä¼°ç»“æœ
            theory_role_file = os.path.join(
                role_output_dir,
                f"{theory_name.replace(' ', '_').lower()}_role_evaluation.json"
            )
            with open(theory_role_file, "w", encoding="utf-8") as f:
                json.dump(role_result, f, ensure_ascii=False, indent=2)
            
            print(f"[INFO] ç†è®º '{theory_name}' çš„è§’è‰²è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {theory_role_file}")
            
        except Exception as e:
            print(f"[ERROR] è¯„ä¼°ç†è®º '{theory_name}' çš„è§’è‰²æ—¶å‡ºé”™: {str(e)}")
            
    # 4. ä¿å­˜ç»¼åˆç»“æœå’Œæ’å
    if role_results:
        # ä¿å­˜è§’è‰²è¯„ä¼°æ±‡æ€»
        role_summary_file = os.path.join(role_output_dir, "role_evaluation_summary.json")
        with open(role_summary_file, "w", encoding="utf-8") as f:
            json.dump(role_results, f, ensure_ascii=False, indent=2)
        
        # è®¡ç®—ç»¼åˆæ’å
        combined_rankings = []
        for role_result in role_results:
            theory_name = role_result.get('theory_name', 'Unknown Theory')
            
            # è®¡ç®—è§’è‰²è¯„ä¼°å¹³å‡åˆ†
            role_scores = []
            if 'evaluations' in role_result and isinstance(role_result['evaluations'], dict):
                for role, eval_data in role_result['evaluations'].items():
                    if isinstance(eval_data, dict) and 'score' in eval_data:
                        score = eval_data['score']
                        if score is not None:
                            role_scores.append(score)
            
            avg_role_score = sum(role_scores) / len(role_scores) if role_scores else 0
            
            # ç»¼åˆè¯„åˆ† = å®éªŒæˆåŠŸç‡ * 0.6 + è§’è‰²è¯„ä¼°åˆ† * 0.4
            combined_score = (
                role_result['experiment_success_rate'] * 0.6 + 
                avg_role_score / 10.0 * 0.4  # è§’è‰²è¯„ä¼°åˆ†é€šå¸¸æ˜¯1-10åˆ†ï¼Œå½’ä¸€åŒ–åˆ°0-1
            )
            
            # æ”¶é›†è§’è‰²è¯¦ç»†è¯„åˆ†
            role_details = {}
            if 'evaluations' in role_result and isinstance(role_result['evaluations'], dict):
                for role, eval_data in role_result['evaluations'].items():
                    if isinstance(eval_data, dict) and 'score' in eval_data:
                        role_details[role] = eval_data.get('score', 0)
            
            combined_rankings.append({
                'theory_name': theory_name,
                'experiment_success_rate': role_result['experiment_success_rate'],
                'average_chi2': role_result['average_chi2'],
                'experiments_count': role_result['experiments_count'],
                'average_role_score': avg_role_score,
                'combined_score': combined_score,
                'role_details': role_details
            })
        
        # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
        combined_rankings.sort(key=lambda x: -x['combined_score'])
        
        # ä¿å­˜ç»¼åˆæ’å
        combined_ranking_file = os.path.join(role_output_dir, "combined_rankings.json")
        with open(combined_ranking_file, "w", encoding="utf-8") as f:
            json.dump(combined_rankings, f, ensure_ascii=False, indent=2)
        
        print(f"\n[INFO] è§’è‰²è¯„ä¼°æ±‡æ€»å·²ä¿å­˜åˆ°: {role_summary_file}")
        print(f"[INFO] ç»¼åˆæ’åå·²ä¿å­˜åˆ°: {combined_ranking_file}")
        
        # æ‰“å°ç»¼åˆæ’å
        print("\n" + "="*120)
        print("ğŸ† ç»¼åˆæ’åï¼ˆå®éªŒæˆåŠŸç‡ 60% + è§’è‰²è¯„ä¼° 40%ï¼‰")
        print("="*120)
        print(f"{'æ’å':<4} {'ç†è®ºåç§°':<40} {'å®éªŒæˆåŠŸç‡':<12} {'è§’è‰²å¹³å‡åˆ†':<12} {'ç»¼åˆè¯„åˆ†':<12} {'è¯¦ç»†è§’è‰²è¯„åˆ†'}")
        print("-"*120)
        
        for i, rank in enumerate(combined_rankings, 1):
            role_detail_items = rank.get('role_details', {}).items()
            role_detail = ", ".join([f"{role_evaluator.evaluation_roles.get(role, {}).get('name', 'æœªçŸ¥')}:{score:.1f}" for role, score in role_detail_items if score > 0])
            print(f"{i:<4} {rank['theory_name']:<40} {rank['experiment_success_rate']*100:>8.1f}% "
                  f"{rank['average_role_score']:>10.1f} {rank['combined_score']:>10.3f}    {role_detail}")
        
        print("-"*120)
        print(f"å…±å®Œæˆ {len(combined_rankings)} ä¸ªç†è®ºçš„è§’è‰²è¯„ä¼°ã€‚")
        
        # è¾“å‡ºæœ€ä½³ç†è®ºçš„è¯¦ç»†ä¿¡æ¯
        if combined_rankings:
            best_theory = combined_rankings[0]
            print(f"\nğŸ¥‡ æœ€ä½³ç†è®º: {best_theory['theory_name']}")
            print(f"   å®éªŒæˆåŠŸç‡: {best_theory['experiment_success_rate']*100:.1f}%")
            print(f"   å¹³å‡Ï‡Â²å€¼: {best_theory['average_chi2']:.4f}")
            print(f"   è§’è‰²è¯„ä¼°å¹³å‡åˆ†: {best_theory['average_role_score']:.1f}/10")
            print(f"   ç»¼åˆè¯„åˆ†: {best_theory['combined_score']:.3f}")
        
        return combined_ranking_file
    else:
        print("[WARNING] æ²¡æœ‰æˆåŠŸå®Œæˆçš„è§’è‰²è¯„ä¼°ç»“æœ")
        return None

async def standalone_auto_role_evaluation(ranking_files: List[str], theories_dir: str, output_dir: str, 
                              success_threshold: float = 0.8, model_source: str = "deepseek", 
                              model_name: str = "deepseek-chat"):
    """
    åŸºäºå®éªŒè¯„ä¼°æ’åç»“æœè¿›è¡Œè‡ªåŠ¨è§’è‰²è¯„ä¼° (ç‹¬ç«‹è¿è¡Œæ¨¡å¼)
    
    Args:
        ranking_files: ä¸€ä¸ªæˆ–å¤šä¸ªç†è®ºæ’åæ–‡ä»¶è·¯å¾„çš„åˆ—è¡¨
        theories_dir: åŒ…å«ç†è®ºå®šä¹‰æ–‡ä»¶çš„ç›®å½•è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        success_threshold: æˆåŠŸç‡é˜ˆå€¼ (é»˜è®¤0.8å³80%)
        model_source: LLMæ¨¡å‹æ¥æº
        model_name: LLMæ¨¡å‹åç§°
    """
    print(f"[INFO] ä»¥ç‹¬ç«‹æ¨¡å¼è¿è¡Œè§’è‰²è¯„ä¼°...")
    print(f"[INFO] æˆåŠŸç‡é˜ˆå€¼: {success_threshold*100:.0f}%")
    
    # 1. è¯»å–å¹¶åˆå¹¶æ‰€æœ‰ç†è®ºæ’åç»“æœ
    all_rankings = []
    for rank_file in ranking_files:
        try:
            with open(rank_file, 'r', encoding='utf-8') as f:
                all_rankings.extend(json.load(f))
            print(f"[INFO] å·²åŠ è½½æ’åæ–‡ä»¶: {rank_file}")
        except Exception as e:
            print(f"[ERROR] è¯»å–æ’åæ–‡ä»¶ {rank_file} å¤±è´¥: {str(e)}")
            continue
    
    if not all_rankings:
        print("[ERROR] æœªåŠ è½½ä»»ä½•æ’åæ•°æ®ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    # 2. è¯»å–ç†è®ºå®šä¹‰
    all_theories_definitions = {}
    try:
        theory_files = [f for f in os.listdir(theories_dir) if f.endswith('.json')]
        print(f"[INFO] åœ¨ç›®å½• '{theories_dir}' ä¸­æ‰¾åˆ° {len(theory_files)} ä¸ªç†è®ºå®šä¹‰æ–‡ä»¶ã€‚")
        for file_name in theory_files:
            file_path = os.path.join(theories_dir, file_name)
            with open(file_path, 'r', encoding='utf-8') as f:
                theory_data = json.load(f)
                theory_name = theory_data.get('name')
                if theory_name:
                    all_theories_definitions[theory_name] = theory_data
                else:
                    print(f"[WARN] æ–‡ä»¶ {file_name} ä¸­ç¼ºå°‘ç†è®ºåç§° 'name'ã€‚")
    except Exception as e:
        print(f"[ERROR] è¯»å–ç†è®ºå®šä¹‰ç›®å½• '{theories_dir}' å¤±è´¥: {str(e)}")
        return

    # 3. å»é‡å¹¶ç­›é€‰é«˜æˆåŠŸç‡ç†è®º
    seen_theories = set()
    high_success_theories = []
    for rank in all_rankings:
        t_name = rank.get('theory_name')
        s_rate = rank.get('success_rate')

        if t_name and s_rate is not None and t_name not in seen_theories:
            if s_rate >= success_threshold:
                high_success_theories.append(rank)
            seen_theories.add(t_name)
    
    # æŒ‰æˆåŠŸç‡å’ŒÏ‡Â²æ’åº
    high_success_theories.sort(key=lambda x: (-x['success_rate'], x.get('average_chi2', float('inf'))))

    # 4. å¦‚æœæœ‰ç¬¦åˆæ¡ä»¶çš„ç†è®ºï¼Œåˆ™è¿è¡Œè§’è‰²è¯„ä¼°
    if high_success_theories:
        await run_role_evaluation_for_theories(
            high_success_theories=high_success_theories,
            all_theories_definitions=all_theories_definitions,
            output_dir=output_dir,
            model_source=model_source,
            model_name=model_name
        )
    else:
        print(f"[INFO] æ²¡æœ‰ç†è®ºè¾¾åˆ° {success_threshold*100:.0f}% çš„æˆåŠŸç‡é˜ˆå€¼ï¼Œè§’è‰²è¯„ä¼°ç»“æŸã€‚")

def main():
    parser = argparse.ArgumentParser(description="è‡ªåŠ¨è¿›è¡Œå¤šè§’è‰²è¯„ä¼°")
    parser.add_argument('ranking_files', nargs='+', help="ä¸€ä¸ªæˆ–å¤šä¸ªå®éªŒè¯„ä¼°æ’åæ–‡ä»¶çš„è·¯å¾„ (JSONæ ¼å¼)")
    parser.add_argument('--theories_dir', required=True, help="åŒ…å«æ‰€æœ‰ç†è®ºå®šä¹‰æ–‡ä»¶çš„ç›®å½•è·¯å¾„")
    parser.add_argument('--output_dir', default="data/role_evaluation_results", help="è¯„ä¼°ç»“æœçš„è¾“å‡ºç›®å½•")
    parser.add_argument('--threshold', type=float, default=0.6, help="è¿›è¡Œè§’è‰²è¯„ä¼°çš„å®éªŒæˆåŠŸç‡é˜ˆå€¼ (ä¾‹å¦‚ 0.6 è¡¨ç¤º 60%)")
    
    # LLM ç›¸å…³å‚æ•°
    parser.add_argument("--model_source", type=str, default="openai", choices=['openai', 'google', 'deepseek'], help="LLM APIçš„æœåŠ¡å•†")
    parser.add_argument("--model_name", type=str, default="gpt-4o-mini", help="å…·ä½“çš„LLMæ¨¡å‹åç§°")
    
    args = parser.parse_args()

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(args.output_dir, exist_ok=True)
    
    asyncio.run(standalone_auto_role_evaluation(
        ranking_files=args.ranking_files,
        theories_dir=args.theories_dir,
        output_dir=args.output_dir,
        success_threshold=args.threshold,
        model_source=args.model_source,
        model_name=args.model_name
    ))

if __name__ == "__main__":
    main()