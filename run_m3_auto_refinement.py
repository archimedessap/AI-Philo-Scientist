#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
run_m3_auto_refinement.py
=========================
ä¸€æ¬¡æ€§å®Œæˆ Top-N ç­›é€‰ + æ·±åº¦ä¼˜åŒ–å¾ªç¯ + ç»“æœæ’è¡Œæ¦œæ±‡æ€»ã€‚

ä½¿ç”¨ç¤ºä¾‹ï¼š
python run_m3_auto_refinement.py \
  --summary_file data/demo/final_evaluation_summary.json \
  --theories_root data/demo/eval_ready_theories \
  --output_dir data/m3_runs \
  --top_n 5 \
  --eval_mode real \
  --max_iters 2
"""

from __future__ import annotations
import os
import sys
import argparse
import time
import json
from pathlib import Path
import subprocess

# -----------------------------------------------------------------------------
# ç¬¬ 0 æ­¥: å·¥å…·å‡½æ•°
# -----------------------------------------------------------------------------

def run_cmd(cmd: list[str]):
    print("\n$ " + " ".join(cmd))
    return subprocess.call(cmd)

# -----------------------------------------------------------------------------
# ä¸»æµç¨‹
# -----------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(description="M3 ä¸€é”®æ·±åº¦ä¼˜åŒ–å¾ªç¯")
    parser.add_argument("--summary_file", required=True, help="final_evaluation_summary.json è·¯å¾„")
    parser.add_argument("--theories_root", required=True, help="eval_ready_theories ç›®å½•")
    parser.add_argument("--output_dir", default="data/m3_runs", help="è¾“å‡ºæ ¹ç›®å½•")
    parser.add_argument("--top_n", type=int, default=5, help="ç­›é€‰å‰ N")
    parser.add_argument("--eval_mode", choices=["quick", "real"], default="quick", help="è¯„ä¼°æ¨¡å¼")
    parser.add_argument("--max_iters", type=int, default=3, help="æ·±åº¦ä¼˜åŒ–æœ€å¤§è¿­ä»£æ¬¡æ•°")
    parser.add_argument("--min_improve", type=float, default=0.05, help="è§†ä¸ºæœ‰æ•ˆæå‡çš„æœ€å° Î” åˆ†")
    parser.add_argument("--judge_model_source", default="deepseek", help="è¯„å®¡ LLM æ¥æº")
    parser.add_argument("--judge_model_name", default="deepseek-reasoner", help="è¯„å®¡ LLM åç§°")
    parser.add_argument("--dialog_model_source", default="deepseek", help="å¯¹è¯ LLM æ¥æº")
    parser.add_argument("--dialog_model_name", default="deepseek-reasoner", help="å¯¹è¯ LLM åç§°")

    args = parser.parse_args()

    # åˆ›å»ºå”¯ä¸€è¾“å‡ºç›®å½•
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    run_root = Path(args.output_dir) / f"run_{timestamp}"
    run_root.mkdir(parents=True, exist_ok=True)
    print(f"[INFO] è¿è¡Œè¾“å‡ºç›®å½•: {run_root}")

    # ---------------------------------------------------------------------
    # æ­¥éª¤ 1: æ‰§è¡Œæ‰¹é‡æ·±åº¦ä¼˜åŒ–
    # ---------------------------------------------------------------------
    depth_output_dir = run_root / "depth_output"
    depth_output_dir.mkdir(exist_ok=True)

    cmd_refine = [
        "python", "run_refinement_loop.py",
        "--summary_file", args.summary_file,
        "--theories_root", args.theories_root,
        "--output_root", str(depth_output_dir),
        "--top_n", str(args.top_n),
        "--min_improve", str(args.min_improve),
        "--eval_mode", args.eval_mode,
        "--max_iters", str(args.max_iters),
        "--judge_model_source", args.judge_model_source,
        "--judge_model_name", args.judge_model_name,
        "--dialog_model_source", args.dialog_model_source,
        "--dialog_model_name", args.dialog_model_name,
    ]

    if run_cmd(cmd_refine) != 0:
        print("[FATAL] æ·±åº¦ä¼˜åŒ–é˜¶æ®µå¤±è´¥ã€‚")
        sys.exit(1)

    # ---------------------------------------------------------------------
    # æ­¥éª¤ 2: ç”Ÿæˆæ’è¡Œæ¦œ (æ ¹æ® depth_runs/*/summary.json)
    # ---------------------------------------------------------------------
    summaries_dir = depth_output_dir / "depth_runs"
    leaderboard = []
    for summary_file in summaries_dir.rglob("summary.json"):
        with open(summary_file, "r", encoding="utf-8") as f:
            data = json.load(f)
            leaderboard.append({
                "theory_name": data["theory_name"],
                "final_score": data["final_score"],
                "iterations": data["iterations"],
                "summary_path": str(summary_file)
            })

    leaderboard.sort(key=lambda x: x["final_score"], reverse=True)
    lb_path = run_root / "leaderboard.json"
    with open(lb_path, "w", encoding="utf-8") as f:
        json.dump(leaderboard, f, ensure_ascii=False, indent=2)

    # åŒæ—¶è¾“å‡º markdown æ–¹ä¾¿æŸ¥çœ‹
    md_lines = ["| Rank | Theory | Score | Iters |", "| ---- | ------ | ----- | ----- |"]
    for idx, item in enumerate(leaderboard, 1):
        md_lines.append(f"| {idx} | {item['theory_name']} | {item['final_score']:.3f} | {item['iterations']} |")
    with open(run_root / "leaderboard.md", "w", encoding="utf-8") as f:
        f.write("\n".join(md_lines))

    print("\nğŸ† æ’è¡Œæ¦œå·²ç”Ÿæˆ:", lb_path)
    print("è¿è¡Œå…¨éƒ¨å®Œæˆï¼ğŸ‰")


if __name__ == "__main__":
    main() 