#!/usr/bin/env python3
# coding: utf-8
"""
åˆå¹¶ã€é‡æ’åºå’Œé‡æ–°è¯„ä¼°å·¥å…·

æ­¤è„šæœ¬ç”¨äºå°†å¤šä¸ªè¯„ä¼°è¿è¡Œ(run)çš„ç»“æœåˆå¹¶ï¼Œå¹¶å¯¹åˆå¹¶åçš„å®Œæ•´æ•°æ®é›†è¿›è¡Œæœ€ç»ˆçš„æ’åºå’Œå¯é€‰çš„è§’è‰²æ‰®æ¼”è¯„ä¼°ã€‚
è¿™åœ¨è¯„ä¼°è¿‡ç¨‹å› ä»»ä½•åŸå› ä¸­æ–­å¹¶ä»æ–­ç‚¹æ¢å¤åç‰¹åˆ«æœ‰ç”¨ã€‚

ç”¨æ³•:
python tools/merge_and_rerank.py \
    --run_dirs evaluation_results/run_20240523_100000 evaluation_results/run_20240523_113000 \
    --output_dir evaluation_results/merged_run_20240523 \
    --theory_path data/theories_v2.1 \
    --run_role_evaluation
"""

import sys
import os
import json
import asyncio
import argparse
import glob
import time
from pathlib import Path

# å°†æ ¹ç›®å½•æ·»åŠ åˆ°sys.pathä»¥å¯¼å…¥é¡¹ç›®æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ä»demoè„šæœ¬ä¸­å¯¼å…¥å¿…è¦çš„å‡½æ•°
from demo.auto_role_evaluation import run_role_evaluation_for_theories
from demo.demo_1 import load_theories_from_sources

def find_summary_files(run_dirs):
    """åœ¨ç»™å®šçš„è¿è¡Œç›®å½•ä¸­æŸ¥æ‰¾æ‰€æœ‰_summary.jsonæ–‡ä»¶"""
    summary_files = []
    for run_dir in run_dirs:
        # ä½¿ç”¨globé€’å½’æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶
        search_pattern = os.path.join(run_dir, "**", "_summary.json")
        found_files = glob.glob(search_pattern, recursive=True)
        if not found_files:
            print(f"[WARN] åœ¨ç›®å½• {run_dir} ä¸­æ²¡æœ‰æ‰¾åˆ° '_summary.json' æ–‡ä»¶ã€‚")
        summary_files.extend(found_files)
    return summary_files

def merge_results(summary_files):
    """åˆå¹¶æ‰€æœ‰æ‰¾åˆ°çš„æ‘˜è¦æ–‡ä»¶ä¸­çš„ç»“æœ"""
    all_results = []
    seen_results = set() # ç”¨äºè·Ÿè¸ªå·²ç»æ·»åŠ çš„ (theory_name, experiment_id) å¯¹ï¼Œé˜²æ­¢é‡å¤

    for f_path in summary_files:
        try:
            with open(f_path, 'r', encoding='utf-8') as f:
                results = json.load(f)
                for result in results:
                    # åˆ›å»ºä¸€ä¸ªå”¯ä¸€æ ‡è¯†ç¬¦æ¥æ£€æµ‹é‡å¤
                    identifier = (result.get('theory_name'), result.get('experiment_id'))
                    if identifier not in seen_results:
                        all_results.append(result)
                        seen_results.add(identifier)
                    else:
                        print(f"[INFO] å‘ç°é‡å¤è¯„ä¼°ç»“æœï¼Œå°†è·³è¿‡: {identifier[0]} vs {identifier[1]}")
        except Exception as e:
            print(f"[ERROR] è¯»å–æˆ–è§£ææ–‡ä»¶ {f_path} æ—¶å‡ºé”™: {e}")
    return all_results

async def main():
    parser = argparse.ArgumentParser(
        description="Merge, Rerank, and Re-evaluate multiple evaluation runs.",
        formatter_class=argparse.RawTextHelpFormatter
    )

    # --- ä¸»è¦å‚æ•° ---
    main_group = parser.add_argument_group('Main Parameters')
    main_group.add_argument("--run_dirs", type=str, nargs='+', required=True, help="List of run directories to merge.")
    main_group.add_argument("--output_dir", type=str, default=None, help="Directory to save the merged results. Defaults to a new directory in 'evaluation_results'.")
    main_group.add_argument("--theory_path", type=str, required=True, help="Path to the original directory of theory definitions, needed for role evaluation.")
    main_group.add_argument("--schema_version", type=str, default="2.1", help="Schema version of theories to load.")

    # --- è§’è‰²è¯„ä¼°å‚æ•° (ä»demo_1.pyå¤åˆ¶) ---
    role_eval_group = parser.add_argument_group('Role-playing Evaluation (Optional)')
    role_eval_group.add_argument("--run_role_evaluation", action='store_true', help="Run role-playing evaluation on the merged and ranked results.")
    role_eval_group.add_argument("--role_success_threshold", type=float, default=0.75, help="Success rate threshold for a theory to be passed to role evaluation.")
    role_eval_group.add_argument("--role_model_source", type=str, default="deepseek", choices=["openai", "deepseek", "google"], help="LLM provider for role evaluation.")
    role_eval_group.add_argument("--role_model_name", type=str, default="deepseek-reasoner", help="Specific model name for role evaluation.")
    
    args = parser.parse_args()

    # --- 1. è®¾ç½®è¾“å‡ºç›®å½• ---
    if args.output_dir:
        output_dir = args.output_dir
    else:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        output_dir = os.path.join("evaluation_results", f"merged_run_{timestamp}")
    
    os.makedirs(output_dir, exist_ok=True)
    print(f"[SETUP] Merged results will be saved in: {output_dir}")

    # --- 2. æŸ¥æ‰¾å¹¶åˆå¹¶ç»“æœ ---
    print(f"\n[MERGE] Searching for summary files in: {args.run_dirs}")
    summary_files = find_summary_files(args.run_dirs)
    if not summary_files:
        print("[ERROR] No summary files found. Exiting.")
        return
    
    print(f"[MERGE] Found {len(summary_files)} summary files. Merging...")
    all_results = merge_results(summary_files)
    print(f"[MERGE] Merged a total of {len(all_results)} unique evaluation results.")

    # --- 3. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š (é€»è¾‘ä»demo_1.pyå¤ç”¨) ---
    if all_results:
        # æ±‡æ€»æ¯ä¸ªç†è®ºçš„è¡¨ç°
        theory_performance = {}
        for result in all_results:
            t_name = result['theory_name']
            if t_name not in theory_performance:
                theory_performance[t_name] = {'success_count': 0, 'total_count': 0, 'chi2_sum': 0, 'chi2_list': []}
            
            theory_performance[t_name]['total_count'] += 1
            
            # ä¼˜å…ˆä½¿ç”¨ä¿®æ­£åçš„æˆåŠŸçŠ¶æ€ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨åŸå§‹æˆåŠŸçŠ¶æ€
            is_success = result.get('success_corrected', result.get('success'))
            if is_success:
                theory_performance[t_name]['success_count'] += 1
            
            chi2 = result.get('chi2_corrected') or result.get('chi2')
            if chi2 is not None:
                theory_performance[t_name]['chi2_sum'] += chi2
                theory_performance[t_name]['chi2_list'].append(chi2)

        # è®¡ç®—æˆåŠŸç‡å’Œå¹³å‡chi2
        ranked_theories = []
        for t_name, perf_data in theory_performance.items():
            success_rate = (perf_data['success_count'] / perf_data['total_count']) if perf_data['total_count'] > 0 else 0
            average_chi2 = (perf_data['chi2_sum'] / len(perf_data['chi2_list'])) if perf_data['chi2_list'] else float('inf')
            ranked_theories.append({
                "theory_name": t_name,
                "success_rate": success_rate,
                "average_chi2": average_chi2,
                "experiments_count": perf_data['total_count']
            })
            
        # æŒ‰æˆåŠŸç‡é™åºï¼Œå¹³å‡chi2å‡åºæ’åº
        ranked_theories.sort(key=lambda x: (-x['success_rate'], x['average_chi2']))

        # ä¿å­˜åˆå¹¶åçš„å®Œæ•´ç»“æœå’Œæ’å
        final_summary_file = os.path.join(output_dir, "final_evaluation_summary.json")
        with open(final_summary_file, "w", encoding="utf-8") as f:
            json.dump(ranked_theories, f, ensure_ascii=False, indent=2)
        
        full_results_file = os.path.join(output_dir, "all_merged_results.json")
        with open(full_results_file, "w", encoding="utf-8") as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)

        print(f"\n[FINAL] Overall experimental evaluation summary saved to: {final_summary_file}")
        print(f"[FINAL] All merged raw results saved to: {full_results_file}")

        # æ‰“å°å®éªŒè¯„ä¼°æ’å
        print("\n" + "="*100)
        print("ğŸ“Š å®éªŒè¯„ä¼°æ’å (åˆå¹¶å)")
        print("="*100)
        print(f"{'æ’å':<5} {'ç†è®ºåç§°':<40} {'æˆåŠŸç‡':<15} {'å¹³å‡Ï‡Â²':<15} {'å®éªŒæ•°':<10}")
        print("-"*100)
        for i, rank in enumerate(ranked_theories, 1):
            print(f"{i:<5} {rank['theory_name']:<40} {rank['success_rate']*100:14.1f}% {rank['average_chi2']:<15.4f} {rank['experiments_count']:<10}")
        print("-"*100)
        
        # --- 4. è‡ªåŠ¨è¿è¡Œè§’è‰²è¯„ä¼° (å¦‚æœå¯ç”¨) ---
        if args.run_role_evaluation:
            print("\n[ROLE-PLAY] Preparing for role-playing evaluation...")
            # åŠ è½½å®Œæ•´çš„ç†è®ºå®šä¹‰
            print(f"[LOAD] Loading full theory definitions from: {args.theory_path}")
            schema_to_load = None if args.schema_version.lower() == 'any' else args.schema_version
            all_theories_definitions = load_theories_from_sources(args.theory_path, schema_version=schema_to_load)
            
            if not all_theories_definitions:
                print("[ERROR] Cannot run role evaluation without theory definitions. Exiting role evaluation.")
                return

            # ç­›é€‰é«˜æˆåŠŸç‡ç†è®º
            high_success_theories = [
                r for r in ranked_theories 
                if r['success_rate'] >= args.role_success_threshold
            ]
            
            if high_success_theories:
                print(f"[ROLE-PLAY] Found {len(high_success_theories)} theories meeting the {args.role_success_threshold*100:.0f}% success threshold.")
                # è°ƒç”¨è§’è‰²è¯„ä¼°æ¨¡å—
                await run_role_evaluation_for_theories(
                    high_success_theories=high_success_theories,
                    all_theories_definitions=all_theories_definitions,
                    output_dir=output_dir, # åœ¨åˆå¹¶åçš„æ–°ç›®å½•ä¸­è¾“å‡º
                    model_source=args.role_model_source,
                    model_name=args.role_model_name
                )
            else:
                print(f"\n[INFO] æ²¡æœ‰ç†è®ºè¾¾åˆ° {args.role_success_threshold*100:.0f}% çš„æˆåŠŸç‡é˜ˆå€¼ï¼Œè·³è¿‡è§’è‰²è¯„ä¼°ã€‚")

    else:
        print("\n[FINAL] No evaluation results found after merging.")

    print("\nMerge and Rerank process finished.")

if __name__ == "__main__":
    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n[INFO] è¿è¡Œè¢«ç”¨æˆ·ä¸­æ–­ã€‚")
    except Exception as e:
        print(f"[CRITICAL ERROR] å‘ç”Ÿæœªå¤„ç†çš„å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc() 